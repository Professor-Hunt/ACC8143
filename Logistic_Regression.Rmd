---
title: "Logistic Regression"
description: |
  Introduction to logistic regression
output: 
  distill::distill_article:
    toc: true
    toc_depth: 3
---

# Game Plan

In previous sections, we have seen how to perform linear regression on input data to predict a continuous value. In some cases, however, we wish to predict a categorical value, such as True/False or Yes/No. Traditional regression methods are not optimal for these problems, since this requires the prediction of a discrete and not continuous value. In this section we introduce a technique that simulates linear regression, but with an additional function employed that maps the continuous value predicted by linear regression methods into a probability, or specifically the range `[0,1]`. In this manner, we can apply a threshold to this probability to predict a binary response.

While several functions might be suitable for this transformation, the most popular function is the logit function. Note that some older analyses might reference the probit function. Performing regression by using the logit function is known as logistic regression (the inverse of the logit function is known as the logistic function). The name might seem confusing since technically this algorithm is used to perform classification, but since logistic regression borrows heavily in its approach from linear regression, the descriptive name was maintained. A major benefit of logistic regression is the creation of a parametric model that can be explored to understand why predictions are made, in the same manner as a linear regression model.

In this section, we introduce the logit function and how it can be used to construct a binary model. Next, we introduce logistic regression, and specifically show how logistic regression can be performed. We also introduce several popular performance metrics and show how they can be calculated for binary classification tasks. We demonstrate logistic regression on several data sets, including one that contains categorical features. We also demonstrate how to perform logistic regression. Finally, we discuss topics such as marginal effects and odds ratios, which are concepts that often prove useful in interpreting logistic regression models.

# Formalism

In a binary classification process, we have two possible outcomes, which for the sake of generality, we can label as *Success* or *Failure*. Denoting the probability of these two outcomes as $P(S)$ and $P(F)$ respectively, we can write the probability of success as $P(S)=p$, and the probability of failure as $P(F)=1−p$. Thus, the odds of a successful outcome, which is the ratio of the probability of success to the probability of failure, is given by the following expression:

$$
\textrm{Odds}(S) = \frac{p}{1 - p}
$$

We can extend the framework of *linear regression* to the task of binary classification by employing a mapping between the continuous value predicted by a linear regressor and the probability of an event occurring, which is bounded by the range $[0,1]$. To do this, we need a function that maps the real numbers into this range, which enables a regression onto a set of discrete values (0 or 1) that provides us the binary classification. One popular choice for this function is the *logit* function, while another choice is the *probit* function. The use of these functions for a classification task leads to *logistic regression* or *probit regression*. While we focus in this section on the application of logistic regression for the binary classification task, this approach can be generalized to classify into more than two categories, this more advanced technique is known as [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression).

## Logit Function

The [*logit* function](https://en.wikipedia.org/wiki/Logit) is defined as the logarithm of the odds (i.e, $p/(1−p)$), which is also known as the *log-odds*. Thus, the *logit* function can be written for a probability of success $p$:

$$
\textrm{logit}(p) = \log\left(\frac{p}{1 - p}\right) ,  where  0 \leq p \leq 1
$$

We can invert this relationship to obtain the [*logistic* function](https://en.wikipedia.org/wiki/Logistic_function), which for a parameter $α$ is defined by the following expression:

$$
\textrm{logit}^{-1}(\alpha) = \textrm{logistic}(\alpha) = \frac{1}{1 + \exp{(-\alpha})}
$$

While the logistic function is most commonly used to perform this type of regression, a related function is the [*probit* function](https://en.wikipedia.org/wiki/Probit), which stands for *probability unit* and is sometimes used in lieu of the *logit* function. The *probit* function is defined for a probability of success, pp:

$$
\textrm{probit}(p) = \sqrt{2}erf^{-1}(2p - 1), \ where \ 0 \leq p \leq 1 \, and \ erf \ is \ the \ Error \ Function.
$$

The logit function (and the probit function) is an *S* shaped curve that converts real numbers into a probability. Both the logit and probit functions are related to the *sigmoid* function, but are centered at the origin (0, 0). For the rest of this section, we will only consider the logit function. In the following Code chunk, we plot the **logistic** function, or the inverse of the **logit** function, demonstrating how the real numbers can be mapped into the range $[0,1]$.

```{r logistic1, echo=TRUE}
#compute x y axis
x<-seq(-10,10,.5)
y<-1/(1+exp(-x))

library(ggplot2)
ggplot(data=as.data.frame(cbind(x,y)),aes(x,y))+geom_line(color="lightblue",size=.75)+
  geom_hline(aes(yintercept=.5),color="black",linetype="dashed") +
  theme_minimal() +
  labs(x=expression(alpha), y=expression(Rho), title='Logistic Function') +
  theme(plot.title = element_text(hjust=0.25, size=15, face='bold'))


```

## Gradient Descent

Given the previously defined *logistic* function, we can develop the formalism of *logistic regression* by first employing a linear regression model to predict a dependent variable from the set of independent features. Second, we apply the logistic function to the dependent variable in the linear regression model to make our binary classification prediction. Thus, if we have the following linear model:

$$y=mx+b$$

The logistic regression model fits the following logistic model:

$$\textrm{logistic}(y) = \frac{1}{1 + \exp(-y)}$$

The generally used cost (or loss) function for logistic regression is the sum of the squared errors between the actual classes and the predicted classes. One of the most popular techniques for finding the minimum of this cost function is to use [*stochastic gradient descent*](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) computes the derivative of (or finds the slope of the tangent line to) the cost function at a particular point. This can be used to modify the parameters of our model to move in a direction that is expected to reach the minimum of the cost function. Standard gradient descent computes these corrections by summing up all the contributions from each training data point. In stochastic gradient descent (or **SGD**), however, the corrections are computed for each training point. As a result, SGD often generates a path towards the minimum that is somewhat rambling, but this has the benefit of avoiding local minima and being more robust.

The following Code cell generates a figure to help explain gradient descent. A fictitious cost function is displayed, along with the tangent (or derivative) at a particular point. The arrows specify the direction that the derivative indicates we must move to reach the minimum. The repeated use of arrows signifies how an incremental approach to gradient descent, such as that employed by stochastic gradient descent, might converge to the true minimum of the cost function.

```{r logitgradient,echo=TRUE,message=FALSE,warning=FALSE}
x<-seq(-4,4,length.out=100)
y=x^2

library(ggplot2)
ggplot(data=as.data.frame(cbind(x,y)),aes(x,y))+
  geom_line(color="lightblue",size=.75)+
  geom_abline(aes(intercept=-1,slope=2),color="red",linetype="dashed")+
  xlim(-1,3)+ylim(-.5,10)+
  geom_point(aes(0,0),pch=4,col="red",size=2,stroke=2)+
  theme_minimal() +
  labs(x=expression(x), y=expression(y), title='Gradient Descent') +
  theme(plot.title = element_text(hjust=0.25, size=15, face='bold'))+ 
  geom_text(x=2.75, y=4, label="Decent")+
  geom_text(x=2.3, y=4, label="Gradient")+
  geom_text(x=0, y=-.5, label="Minimum")


```

```{r gradient, echo=FALSE, include=FALSE}
# Generate random data
x <- runif(500, -4, 4)
y <- x + rnorm(500) + 2.5
# Define the squared error cost function
cost <- function(X, y, theta) {
sum( (X %*% theta - y)^2 ) / (2*length(y))
}
 alpha <- 0.1 # Specify the learning rate
 num_iters <- 1000 # Specify the number of iterations
 cost_history <- rep(0,num_iters) # will be used to store the value of cost function after
# every iteration
 theta_history <- list(num_iters) # will be used to store the value of theta after every
# iteration
 theta <-  c(0,0) # Initial values of theta
 X <- cbind(1,x) # Add a column vector with all values  to be 1 to x so that hypothesis
# function has an intercept
 for (i in 1:num_iters) {
   theta[1] <- theta[1] - alpha * (1/length(y)) * sum(((X%*%theta)- y))
   theta[2] <- theta[2] - alpha * (1/length(y)) * sum(((X%*%theta)- y)*X[,2])
   cost_history[i] <- cost(X, y, theta)
   theta_history[[i]] <- theta
 }
 print(theta)
# Plots the training dataset
 plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Linear regression by gradient descent')
# Plots various lines during the course of convergence
 for (i in c(1,3,6,10,14,seq(20,num_iters,by=10))) {
 abline(coef=theta_history[[i]], col=rgb(0.8,0,0,0.3))
 }
 abline(coef=theta, col='blue') # Plots a straight line with intercept as theta[1] and slope
# as theta[2]

```

Another example of Gradient Descent[^1]

[^1]: This code is borrowed from <https://www.machinegurning.com/rstats/gradient-descent/>

Consider the following function:

$$ h_{\theta}=1.2(x-2)^2 + 3.2 $$

So we can state our objective to minimize $\theta_1$ or $\min\limits_{\theta_1}J(\theta_1)$ and $\theta_1\in\mathbb{R}$

### Cost function

We define the cost function $J(\theta_1)$ using calculus as $J(\theta)=2.4(x−2)$

$$
\begin{multline}
\text{repeat until convergence} \{\\
 
\theta_1:=\theta_1 - \alpha\frac{d}{d\theta_1}J(\theta_1)\\
 
\}
\end{multline}
$$

-   where $α$ is the learning rate governing the size of the step take with each iteration.

```{r gradient_borrowed, echo=TRUE}

library(dplyr)
library(magrittr)
library(ggplot2)
 
 
xs <- seq(0,4,len = 100) # create some values
 
# define the function we want to optimize
 
f <-  function(x) {
  1.2 * (x-2)^2 + 3.2
  }
 
# plot the function 
 
create_plot <- function(title) {
  plot(
    ylim = c(3,8),
    x = xs,
    y = f(xs), 
    type = "l", 
    ylab = expression(1.2(x-2)^2 + 3.2),
    xlab = "x",
    main = title
    )
  
  abline(
    h = 3.2,
    v = 2, 
    col = "red", 
    type = 2
    )
  
}
 
# J(theta)
 
cost <- function(x){
  1.2 * 2 * (x-2)
}
```

Below is the actual implementation of gradient descent.

```{r gradient_borrowed2}
# gradient descent implementation
 
grad <- function(x = 0.1, alpha = 0.6, j = 1000) {
  
  xtrace <- x
  ftrace <- f(x)
  
  for (i in 1:j) {
    
    x <- x - alpha * cost(x)
    
    xtrace <- c(xtrace,x)
    ftrace <- c(ftrace,f(x))
    
    }
  
  data.frame(
    "x" = xtrace,
    "f_x" = ftrace
    )
  }

```

Now I use the plotting function to produce plots, and populate these with points using the gradient descent algorithm.

```{r gradient_borrowed3}

create_plot(expression(Low~alpha))
 
with(
  alpha_too_low <- grad(
    x = 0.1, # initialisation of x
    alpha = 0.1, # learning rate
    j = 100 # iterations
    ),
  points(
    x, 
    f_x, 
    type = "b", 
    col = "green"
    )
  )

create_plot(expression(alpha~just~right))
 
with(
  alpha_just_right <- grad(
    x = 0.1, # initialisation of x
    alpha = 0.6, # learning rate
    j = 100 # iterations
    ),
  points(
    x, 
    f_x, 
    type = "b",  
    col = "blue"
    )
  )

create_plot(expression(High~alpha))
 
with(
  alpha_too_high <- grad(
    x = 0.1, # initialisation of x
    alpha = 0.8, # learning rate
    j = 100 # iterations
    ),
  points(
    x, 
    f_x, 
    type = "b",
    col = "red"
    )
  )

```

Another way to look at the rate of convergence is to plot the number of iterations against the output of $f(x)$. Vertical lines show when convergence occurs. When $\alpha$ is set very low, it takes much longer than necessary (although it does converge). When $\alpha$ is too high, convergence doesn't occur at all within a hundred iterations.

```{r gradient_borrowed4}

par(mfrow=c(1,3))
 
plot(alpha_too_low$x, type = "l",col = "green")
abline(v = (round(alpha_too_low$x,4) != 2) %>% which %>% length)
 
plot(alpha_just_right$x, type = "l",col = "blue")
abline(v = (round(alpha_just_right$x,4) != 2) %>% which %>% length)
 
plot(alpha_too_high$x, type = "l", col = "red")
abline(v = (round(alpha_too_high$x,4) != 2) %>% which %>% length)
```

# Logistic Modeling

Before introducing logistic regression, we first show how the logistic function can be used to model binary response data. For this purpose, we will use data from NASA on the relationship between the outside temperature when the space shuttle was launched, and the occurrence of a thermal failure of an O-ring on a booster rocket. We will use this data to create a predictive model between temperature and thermal failure; note that it is believed that the [failure of an O-ring](https://en.wikipedia.org/wiki/Space_Shuttle_Solid_Rocket_Booster#Challenger_disaster) on a solid rocket booster led to the Challenger disaster.

The actual data we use is hosted at the University of California at Irvine (UCI) machine learning data repository.

```{r getdata,echo=TRUE,eval=FALSE}
# install.packages("devtools")
devtools::install_github("tyluRp/ucimlr")

```

```{r echo=TRUE}
library(dplyr)
#its two datasets...pick the first one in the list
NASA<-ucimlr::challenger[[1]]

knitr::kable(psych::describe(NASA))%>%
    kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width="100%",height="200px")
  
```

------------------------------------------------------------------------

From this summary description we can identify several important points. First, there are 23 instances in this data set. Second, this summary indicates that there are no missing values, since each feature has the same number in the `n` column, and the `min` column always contains a valid number. On the other hand, we notice that the maximum value for the number of thermal distresses is two, not one like we require for a binary classification task, which should be zero or one. Thus, our next step is to determine how many instances have a value of two for the `thermal_distress` feature.

```{r checktarge,echo=TRUE}
table(NASA$thermal_distress)
```

As the output demonstrates, we only have one instance that records more than one thermal distress. At this point we have three options:

1.  delete this instance,

2.  duplicate this instance so that two single failure instances exist in the data set, or

3.  change this instance to report only a single thermal distress.

While any of these options might be valid from an algorithmic perspective, they differ from a modeling perspective. The first option would remove valuable data from our set, which is already small. This would also make our model less predictive since this was an actual failure, and we likely do not want to under predict failures, which could have devastating effects. On the other hand, duplicating this instance would be the same as having two separate launches at the same temperature. This could also be problematic, as it would overemphasize a failure at a given temperature.

As a result, we will instead convert this instance to a single thermal distress. The reason in this case is that this measurement did find an instance of a thermal distress, and we are creating a model between temperature and the probability of a thermal failure.

```{r fixtarget,echo=TRUE}

NASA2<-NASA%>%
  mutate(thermal_distress=ifelse(thermal_distress>1,1,thermal_distress))

table(NASA2$thermal_distress)
```

```{r runlogit, echo=TRUE, message=FALSE, warning=FALSE}

#fit logistic regression model
model <- glm(factor(thermal_distress) ~ launch_temp, data=NASA2, family=binomial)

#define new data frame that contains predictor variable
newdata <- data.frame(launch_temp=seq(min(NASA2$launch_temp), max(NASA2$launch_temp),len=500))

#use fitted model to predict values of vs
newdata$thermal_distress = predict(model, newdata, type="response")

#plot logistic regression curve
plot(thermal_distress ~ launch_temp, data=NASA2, col="steelblue")
lines(thermal_distress ~ launch_temp, newdata, lwd=2)


#using ggplot
library(ggplot2)

#plot logistic regression curve
ggplot(NASA2, aes(x=launch_temp, y=thermal_distress)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))+
  theme_minimal() +
  labs(x="launch temperature", y="thermal distress", title='logistic regression') +
  theme(plot.title = element_text(hjust=0.25, size=15, face='bold'))
```

Given a predictive model such as our computed logit model, we can also predict for new, unseen data. In this case, we can predict the probability of thermal failure for a given temperature. The following Code cell computes and displays these probabilities as a function of temperature. Note, that the temperature at launch during the Challenger disaster was 36 degrees Fahrenheit.

```{r putinvalues, echo=TRUE}

for (i in c(36, 58, 65, 72, 88)){
 
  print(paste(i,round(predict(model,data.frame(launch_temp=i),type="response"),3)))

  }

```

## Logistic Regression

```{r logisticreg, echo=TRUE, warning=FALSE, message=FALSE}
library(caret)
#set the seed :)
set.seed(1)
#get our samples

#lets split the data 60/40

trainIndex <- createDataPartition(NASA2$thermal_distress, p = .6, list = FALSE, times = 1)

#look at the first few
#head(trainIndex)

#grab the data
logisticTrain <- NASA2[ trainIndex,]
logisticTest  <- NASA2[-trainIndex,]


NASA_logistic <- train(
  form = factor(thermal_distress) ~ launch_temp,
  data = NASA2,
  trControl = trainControl(method = "cv", number = 10),
  method = "glm",
  family = "binomial"
)

NASA_logistic

summary(NASA_logistic)

NASA_Pred<-predict(NASA_logistic,logisticTest,type="prob")

knitr::kable(NASA_Pred)

testpred<-cbind(NASA_Pred$`1`,logisticTest)

testpred<-testpred%>%
  rename(NASA_Pred="NASA_Pred$`1`")

plot(pROC::roc(testpred$thermal_distress,testpred$NASA_Pred))

title(paste("AUC ",round(pROC::roc(testpred$thermal_distress,testpred$NASA_Pred)$auc,3)))

testpred<-testpred%>%
  mutate(prediction=if_else(NASA_Pred>.5,1,0))

confusionMatrix(factor(testpred$prediction),factor(testpred$thermal_distress),positive = "1")
```

```{r confusion, fig.width=7, fig.height=7}
ConfusionTableR::binary_visualiseR(train_labels = factor(testpred$prediction),
                                   truth_labels= factor(testpred$thermal_distress),
                                   class_label1 = "Thermal Distress", 
                                   class_label2 = "Prediction",
                                   quadrant_col1 = "#5D1725", 
                                   quadrant_col2 = "#777777", 
                                   custom_title = "Logistic Confusion Matrix", 
                                   text_col= "black")
```

# Exercise 1

Using the code above:

1.  Try including another feature in the model, such as `leak_check_pressure` or `temporal_order_of_flight`, one at a time. Do either of these features improve the performance?
2.  Discuss at least 3 difference performance measures from the confusion matrix results from number 1.

```{r logisticwithpred,echo=TRUE, warning=FALSE, message=FALSE}

#plot logistic regression curve
ggplot(NASA2, aes(x=launch_temp, y=thermal_distress)) + 
  geom_point(alpha=.5,color="lightblue") +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial),color="lightgreen",linetype = "dashed")+
  theme_minimal() +
  labs(x="launch temperature", y="thermal distress", title='logistic regression') +
  theme(plot.title = element_text(hjust=0.25, size=15, face='bold'))+
  geom_line(data=testpred,aes(x=launch_temp,y=prediction),color="pink",size=1,linetype = "dashed")
```

# SGD Classifier

We now turn to the alternative technique for performing logistic regression. We can employ a stochastic gradient descent classifier to perform logistic regression. This has a benefit over the standard logistic regression estimator. This technique employs stochastic gradient descent, which can be [*efficient*]{.ul} in finding the minimum of the cost function, especially with large and complex data sets. Stochastic gradient descent is based on standard gradient descent that was demonstrated graphically earlier, but is less prone to being trapped in local minima (or valleys).

```{r sgd}

#install.packages("sgd")
library(sgd)

sgdmodel<-sgd(formula=factor(thermal_distress) ~ launch_temp,
              data=logisticTrain,
              model="glm",
              model.control=list(family="binomial"),
              sgd.control=list(method="sgd")
              )

coef(sgdmodel)

summary(sgdmodel)

#fitted(sgdmodel)
#plot(sgdmodel)
#print(sgdmodel)

NASA_Pred_sgd<-as.data.frame(predict(sgdmodel,as.matrix(logisticTest$launch_temp),type = "response"))

knitr::kable(NASA_Pred_sgd)

testpredsgd<-cbind(NASA_Pred_sgd$V1 ,logisticTest)

testpredsgd<-testpredsgd%>%
  rename(NASA_Pred_sgd="NASA_Pred_sgd$V1")

plot(pROC::roc(testpredsgd$thermal_distress,testpredsgd$NASA_Pred_sgd))

title(paste("AUC ",round(pROC::roc(testpredsgd$thermal_distress,testpredsgd$NASA_Pred_sgd)$auc,3)))

```

# LR: Tips Data

While the O-ring data was informative, it was small in both number of features and instances. As a result, we will now transition to the *tips* data. With this data set, we will first demonstrate logistic regression using only numeric features before including categorical features as well.

```{r lrtip}


library(curl)

load(curl("https://raw.githubusercontent.com/Professor-Hunt/ACC8143/main/data/tips.rda"))

#view the whole dataset
knitr::kable(tips)%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "100%",height="300px")
```

## Numeric 

To perform regression on the numeric features, we need both the numerical features and a target feature. For this example, we will attempt to predict if a patron is a smoker or not based solely on the three numerical features.

First, we separate the data into training and testing sets.

```{r lrtips2}

#set the seed :)
set.seed(1)
#get our samples

#lets split the data 60/40
library(caret)
trainIndex <- createDataPartition(tips$tip, p = .6, list = FALSE, times = 1)

#look at the first few
#head(trainIndex)

#grab the data
tipsTrain <- tips[ trainIndex,]
tipsTest  <- tips[-trainIndex,]
```

Run the model

```{r lrtips3 }


lrtips <- train(
  form = smoker~total_bill+tip+size,
  data = tipsTrain,
  trControl = trainControl(method = "cv", number = 10),
  method = "glm",
  family = "binomial"
)

lrtips

summary(lrtips)

lrtips_Pred<-predict(lrtips,tipsTest,type="prob")


lrtipstestpred<-cbind(lrtips_Pred,tipsTest)

lrtipstestpred<-lrtipstestpred%>%
  rename(lrtips_Pred="Yes")

plot(pROC::roc(lrtipstestpred$smoker,lrtipstestpred$lrtips_Pred))

title(paste("AUC ",round(pROC::roc(lrtipstestpred$smoker,lrtipstestpred$lrtips_Pred)$auc,3)))

lrtipstestpred<-lrtipstestpred%>%
  mutate(prediction=if_else(lrtips_Pred>.5,1,0),
         target=if_else(smoker=="Yes",1,0))

#confusionMatrix(factor(as.character(lrtipstestpred$prediction)),factor(as.character(lrtipstestpred$target)),positive = "1")
```

```{r, echo=FALSE}


binary_visualiseR2 <- function(train_labels,truth_labels, class_label1="Class Negative",
                              class_label2="Class Positive", quadrant_col1='#3F97D0',
                              quadrant_col2='#F7AD50', custom_title="Confusion matrix",
                              info_box_title="Confusion matrix statistics",
                              text_col="black", round_dig=2, cm_stat_size=1.4, cm_stat_lbl_size=1.5){

  cm <- caret::confusionMatrix(train_labels, truth_labels, positive="1")
  #Define globals
  layout(matrix(c(1,1,2)))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  #n is specified in plot to indicate no plotting
  title(custom_title, cex.main=2)
  # Create the matrix visualisation using custom rectangles and text items on the chart
  rect(150, 430, 240, 370, col=quadrant_col1)
  text(195, 435, class_label1, cex=1.2)
  rect(250, 430, 340, 370, col=quadrant_col2)
  text(295, 435, class_label2, cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col=quadrant_col1)
  rect(250, 305, 340, 365, col=quadrant_col2)
  text(140, 400, class_label1, cex=1.2, srt=90)
  text(140, 335, class_label2, cex=1.2, srt=90)

  #Add the results of the confusion matrix - as these will be saved to cm$table
  result <- as.numeric(cm$table)
  text(195, 400, result[1], cex=1.6, font=2, col=text_col)
  text(195, 335, result[2], cex=1.6, font=2, col=text_col)
  text(295, 400, result[3], cex=1.6, font=2, col=text_col)
  text(295, 335, result[4], cex=1.6, font=2, col=text_col)

  #Add in other confusion matrix statistics
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = info_box_title, xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=cm_stat_lbl_size, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), round_dig), cex=cm_stat_size)
  text(30, 85, names(cm$byClass[2]), cex=cm_stat_lbl_size, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), round_dig), cex=cm_stat_size)
  text(50, 85, names(cm$byClass[5]), cex=cm_stat_lbl_size, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), round_dig), cex=cm_stat_size)
  text(65, 85, names(cm$byClass[6]), cex=cm_stat_lbl_size, font=2)
  text(65, 70, round(as.numeric(cm$byClass[6]), round_dig), cex=cm_stat_size)
  text(86, 85, names(cm$byClass['Balanced Accuracy']), cex=1.6, font=2)
  text(86, 70, round(as.numeric(cm$byClass['Balanced Accuracy']), round_dig), cex=cm_stat_size)

  # add in the accuracy information
  text(30, 35, names(cm$overall[1]), cex=cm_stat_lbl_size, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), round_dig), cex=cm_stat_size)
  text(70, 35, names(cm$overall[2]), cex=cm_stat_lbl_size, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), round_dig), cex=cm_stat_size)
}
```

```{r lrtips4 ,echo=FALSE,fig.width=7, fig.height=7}

binary_visualiseR2(train_labels = factor(as.character(lrtipstestpred$prediction)),
                                   truth_labels= factor(as.character(lrtipstestpred$target)),
                                   class_label1 = "Yes", 
                                   class_label2 = "No",
                                   quadrant_col1 = "#5D1725", 
                                   quadrant_col2 = "#777777", 
                                   custom_title = "Logistic Tips Confusion Matrix", 
                                   text_col= "black")
```

In this simple example, our model provides a reasonable performance, except for the excess *false positives* where we over predict smokers to be non-smokers. While there are several techniques we could explore to improve the performance of this simple model, the easiest will be to include additional features in our model. This will require including categorical features in our model, which is discussed in the next section.

## Categorical and Numeric

Admittedly, our attempt to predict whether a patron is a smoker or not, based solely on the data in the *tips* data set, is ambitious. Even if the model fails to perform sufficiently well, we may gain insight into our data and the relationship between the features. As a result, we now transition to include categorical features in the model.

### Run Model

```{r lrtipsall}

lrtips <- train(
  form = smoker~total_bill+tip+size+factor(sex)+factor(day)+factor(time),
  data = tipsTrain,
  trControl = trainControl(method = "none"),
  method = "glm",
  family = "binomial"
)

lrtips

summary(lrtips)

lrtips_Pred<-predict(lrtips,tipsTest,type="prob")


lrtipstestpred<-cbind(lrtips_Pred,tipsTest)

lrtipstestpred<-lrtipstestpred%>%
  rename(lrtips_Pred="Yes")

plot(pROC::roc(lrtipstestpred$smoker,lrtipstestpred$lrtips_Pred))

title(paste("AUC ",round(pROC::roc(lrtipstestpred$smoker,lrtipstestpred$lrtips_Pred)$auc,3)))

lrtipstestpred<-lrtipstestpred%>%
  mutate(prediction=if_else(lrtips_Pred>.5,1,0),
         target=if_else(smoker=="Yes",1,0))
```

### Results

```{r lrtipsall2 ,echo=FALSE,fig.width=7, fig.height=7}

binary_visualiseR2(train_labels = factor(as.character(lrtipstestpred$prediction)),
                                   truth_labels= factor(as.character(lrtipstestpred$target)),
                                   class_label1 = "Yes", 
                                   class_label2 = "No",
                                   quadrant_col1 = "#5D1725", 
                                   quadrant_col2 = "#777777", 
                                   custom_title = "Logistic Tips Confusion Matrix", 
                                   text_col= "black")
```

### Variable importance

```{r lrtipsall3}

V<-caret::varImp(lrtips)$importance%>%
  arrange(desc(Overall))

knitr::kable(V)

ggplot2::ggplot(V, aes(x=reorder(rownames(V),Overall), y=Overall)) +
geom_point( color="blue", size=4, alpha=0.6)+
geom_segment( aes(x=rownames(V), xend=rownames(V), y=0, yend=Overall), 
color='skyblue') +
xlab('Variable')+
ylab('Overall Importance')+
theme_light() +
coord_flip() 
```

### Regression Output

```{r lrtipsall4,  echo=TRUE, results='asis'}

stargazer::stargazer(glm(factor(smoker)~total_bill+tip+size+factor(sex)+factor(day)+factor(time),data=tipsTrain,family = "binomial"),
                     title="Tips Logistic Regression",
                     type = "html",
                     float = TRUE,
                     report = "vcs*",
                     no.space = TRUE,
                     header=FALSE,
                     single.row = TRUE,
                    #font.size = "small",
                     intercept.bottom = F)
```

# 
Exercise 2

1.  Try predicting sex rather than the `smoker` feature by using all other features. Does the new model perform better or worse than the current model?
2.  What was the most important variable in 1? Why do you think that is?

## **Marginal Effects**

One challenge when building a logistic model with multiple independent features is understanding the *true* effect of any one independent feature. To estimate these effects, one can compute the *marginal effects*, which quantifies the impact one feature has on the prediction while the other features are held constant. Formally this is done by taking the partial derivative of the model with respect to the feature of interest.

## **Odds Ratio**

While marginal effect works well for numeric features, we can't take the derivative of a categorical, or discrete feature. Thus, we can also compute the *odds ratio* to determine the association between the presence or absence of two features. To compute an odds ratio, we hold one categorical value fixed, which is the reference group. This reference category was determined earlier when we held one categorical option out of the fit. Thus, when we calculate an odds ratio for the female instances, it is done with respect to the `sex=male` reference group.

We can use the odds ratio to determine the association, for example, between the `day` feature and the `time` feature, in order to determine if there is a relationship between these two features and the likelihood a patron is a smoker. The value of the odds ratio indicates how much more likely a patron is to be a smoker. We can also compute confidence intervals around these values to quantify the degrees of uncertainty associated with this ratio.

```{r marg1, eval=FALSE}

install.packages("mfx")
```

```{r marg2}
#standard logistic regression
regular_logit<-glm(factor(smoker)~total_bill+tip+size+factor(sex)+factor(day)+factor(time),data=tipsTrain,family = "binomial")
summary(regular_logit)
#partial effects at the mean
Partial_atmean_logit<-mfx::logitmfx(factor(smoker)~total_bill+tip+size+factor(sex)+factor(day)+factor(time),data=tipsTrain,atmean = TRUE)
Partial_atmean_logit
#average partial effects
Avg_Partial_logit<-mfx::logitmfx(factor(smoker)~total_bill+tip+size+factor(sex)+factor(day)+factor(time),data=tipsTrain,atmean = FALSE)
Avg_Partial_logit
#odds Ratio
OR_logit<-mfx::logitor(factor(smoker)~total_bill+tip+size+factor(sex)+factor(day)+factor(time),data=tipsTrain)
OR_logit
```

```{r marg3,eval=FALSE}

install.packages("huxtable")

```

```{r marg4, results='asis'}

OR_logit2<-OR_logit
class(OR_logit2)<-class(Partial_atmean_logit)
OR_logit2$mfxest<-OR_logit2$oddsratio

  fits <- list("Regular"=regular_logit, "Odds Ratio"=OR_logit2, 
             "Partial at the mean"=Partial_atmean_logit, "Avg Partial"=Avg_Partial_logit)

huxtable::huxreg(fits,
      # statistics = c(N = "nobs", R2 = "r.squared"),
       note = "Note.This is cool.")%>%
  huxtable::as_flextable() 
```

fin
