---
title: "Support Vector Machines"
description: |
  Introduction to Support Vector Machines
output: 
  distill::distill_article:
    toc: true
    toc_depth: 3
---

# Support Vector Machine

In this module, we introduce the [Support Vector Machine (SVM) algorithm](https://en.wikipedia.org/wiki/Support_vector_machine), a powerful, but simple supervised learning approach to predicting data. For classification tasks, the SVM algorithm attempts to divide data in the feature space into distinct categories. By default, this division is performed by constructing hyperplanes that optimally divide the data. For regression, the hyperplanes are constructed to map the distribution of data. In both cases, these hyperplanes map linear structures in a non-probabilistic manner. By employing a *kernel trick*, however, we can transform non-linear data sets into linear ones, thus enabling SVM to be applied to non-linear problems.

SVMs are powerful algorithms that have gained widespread popularity. This is due partly to the fact that they are effective in high dimensional feature spaces, including those problems where the number of features is similar to or slightly exceeds the number of instances. They can also be memory efficient since only the support vectors are needed to compute the hyperplanes. Finally, by using different kernels, SVM can be applied to a wide range of learning tasks. On the other hand, these models are black boxes, and it can be difficult to explain how they operate, especially on new instances. They do not, by default, provide probability estimates, since the hyperplane is constructed to cleanly divide the training data.

In this module, we first explore the basic formalism of the SVM algorithm, including the construction of hyperplanes and the kernel trick, which enables SVM to be applied to non-linear problems. Next, we explore the application of SVM to classification problems, which is known as support vector classification, or SVC. To introduce this topic, we will once again use the Iris data to construct an SVC estimator, explore the resulting performance and decision surface, before looking at the effect of different hyperparameter values. Next, we will switch to a more complex data set, the adult data demonstrated in the Introduction to Decision Tree module, with which we will explore unbalanced classes and more advanced classification performance metrics such as the ROC, AUC, and Lift curve. Finally, we will apply SVM to regression problems, which is known as support vector regression. For this we will use the automobile miles per gallon regression task first presented in the Introduction to Decision Tree module.

## **Formalism**

As was the case with the decision tree, one of the simplest machine learning algorithms to understand and employ is the [support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine). For classification tasks, this algorithm simply divides the data with hyperplanes into the resulting classes, while for regression, the hyperplanes form a predictive model of the underlying data. These hyperplanes, by default, produce a linear classifier (or regressor) since they are restricted to be linear in the features. However, unlike a decision tree, SVM produces a black box model; we can't examine the model, especially in higher dimensions, to understand why specific predictions are made. In order to construct the optimal set of hyperplanes, SVM assumes the features are normalized and can be compared equally. Thus, for proper use of an SVM on a data set, we must normalize the features.

Given a set of data with $n$ features, we can construct many different hyperplanes that divide the data. The SVM algorithm selects the optimal hyperplane by finding the one that produces the largest separation, which is known as the *margin*, between the data. The hyperplane that accomplishes this goal is known as the maximum-margin hyperplane. For high dimensional data, a set of hyperplanes is constructed, which accomplishes this same task. In cases where the data cannot be cleanly separated, many SVM implementations map the data set into a higher dimensional space by using a kernel function, where the data are linearly separated and construct a set of optimal hyperplanes in this space. This process can also be used to transform a non-linear feature space into a linear (or approximately linear) space where traditional SVM can be applied.

In the rest of this section, we demonstrate the construction of hyperplanes by using the Iris data set. To simplify the visualization of these data and the resulting hyperplanes, we use only two dimensions. Since SVM, by default, provides a linear classification, these hyperplanes will generate linear divisions between classes. After this, we demonstrate how a kernel can be employed to transform a non-linear problem into a linear classification task.

## **Hyperplanes**

To demonstrate how hyperplanes can divide data, we will use the standard Iris classification data set. We first use our helper functions to load the Iris data and subdivide into training and testing. We normalize the data by using the training function. Next, we select only two dimensions: Sepal Width and Petal Width, to use in our subsequent analysis to enable easier visualization of the training data, test data, and hyperplanes.

There Code chunk uses the training data to generate an SVC (don't worry about the details of doing this right now, they are introduced in the next section). Next, we make a scatter plot of the training data, colored by their label, and display test data with a different symbol. Next, we generate a grid of points through this space and apply the predetermined SVC to generate decisions over this grid (note, this is similar to how we construct decisions surfaces for classification tasks). Finally, the algorithm generates a separate hyperplane to divide between each set of classes. The support vectors used to compute these hyperplanes from the training data.

The hyperplanes shown in the plot are denoted by the solid gray line. We also plot the confidence interval (or upper and lower one-sigma standard deviations) for these hyperplanes, the upper boundary as a blue dashed line, and the lower boundary as a red dashed line. The confidence interval in this case provides an estimate for the uncertainty in the location of the true hyperplane given these training data.

The support are those training data that are used to finalize the selection of the best hyperplane. The training data that anchor the support vectors are enclosed in diamonds. The vector extends from these points to the hyperplane (forming a right angle to the hyperplane). The margin for each support vector is the distance from the support to the hyperplane (or the length of the support vector). It is the combinations of these distances that we seek to minimize when we compute the best hyperplane.

```{r svm1}

library(caret)
library(tidyverse)
#set the seed :)
set.seed(1)
#get our samples
#using the iris data
#lets split the data 60/40

trainIndex <- createDataPartition(iris$Species, p = .6, list = FALSE, times = 1)

#look at the first few
#head(trainIndex)

#grab the data
SVMTrain <- iris[ trainIndex,]
SVMTest  <- iris[-trainIndex,]


iris_SVM <- train(
  form = factor(Species) ~ .,
  data = SVMTrain,
  #here we add classProbs because we want probs
  trControl = trainControl(method = "cv", number = 10,
                           classProbs =  TRUE),
  method = "svmLinear",
  preProcess = c("center", "scale"),
  tuneLength = 10)

iris_SVM

summary(iris_SVM)

svm_Pred<-predict(iris_SVM,SVMTest,type="prob")

knitr::kable(svm_Pred)%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "100%",height="300px")

svmtestpred<-cbind(svm_Pred,SVMTest)

svmtestpred<-svmtestpred%>%
  mutate(prediction=if_else(setosa>versicolor & setosa>virginica,"setosa",
                            if_else(versicolor>setosa & versicolor>virginica, "versicolor",
                                    if_else(virginica>setosa & virginica>versicolor,"virginica", "PROBLEM"))))

table(svmtestpred$prediction)

confusionMatrix(factor(svmtestpred$prediction),factor(svmtestpred$Species))
```

```{r svmplot1}

supportvectors<-SVMTrain[iris_SVM$finalModel@SVindex,]

ggplot(data=SVMTest, mapping = aes(x=Sepal.Width,y=Petal.Width,color=Species))+
  geom_point(alpha=0.5)+
  geom_point(data=svmtestpred, mapping = aes(x=Sepal.Width,y=Petal.Width, color=prediction),shape=6,size=3)+
    geom_point(data=supportvectors, mapping = aes(x=Sepal.Width,y=Petal.Width),shape=4,size=4)+
  theme(legend.title = element_blank())+ggtitle("SVM Demonstration")

```

```{r svmplot2}

svmfit = e1071::svm(Species~., data = SVMTrain, kernel = "linear", cost = 1, scale = TRUE)

# Plot Results
plot(svmfit, SVMTrain, Petal.Width ~ Sepal.Length,
     slice = list(Sepal.Width = 3, Petal.Length = 4))
```

```{r svmplot3, echo=FALSE}

set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)

library(e1071)

dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)

plot(svmfit, dat)

make.grid = function(x, n = 75) {
  grange = apply(x, 2, range)
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}

xgrid = make.grid(x)
xgrid[1:10,]

ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)

beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho

plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)


```

```{r svmplot4}

# construct larger random data set
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2.5
x[101:150,] <- x[101:150,] - 2.5
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")

library(e1071)

# set pseudorandom number generator
set.seed(123)
# sample training data and fit model
train <- base::sample(200,100, replace = FALSE)
svmfit <- svm(y~., data = dat[train,],  kernel = "linear", cost = 10, scale = FALSE)
# plot classifier
plot(svmfit, dat)
```

\

## **Non-Linear Kernels**

In many real-world cases, the data of interest are non-linear. In these cases, we can still successfully employ SVM by transforming the data into a space where the data are linearly separated. This process is known as the *kernel trick*, since we employ a kernel function to perform the mapping.

```{r svmplot5}

svmfit <- svm(y~., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
# plot classifier
plot(svmfit, dat)
```

[Options for SVM](https://topepo.github.io/caret/available-models.html)

# Exercise 1

Use the first code chunk with the iris data.

1.  Change the training to testing split size, for example change from a 60%:40% to a 75%:25%, and to a 50%:50%. Compare the results to the 60/40.

2.  Change the Kernel...use svmPoly for the method. `method = "svmLinear",` to `method = "svmPoly",` compare the linear results with the polynomial results.

## **SVM: Hyperparameters**[^1]

[^1]: <https://uc-r.github.io/svm>

Perhaps the most important hyperparameter for SVC is the `kernel` hyperparameter, which specifies the type of transformation that should be applied to the training data to determine the optimal set of hyperplanes. In the previous example, we computed and displayed the decision surface for a linear kernel. n the following Code chunks, we compute and display decision surfaces for SVCs that employ different kernel functions: `linear`, `polynomial`, `radial`, and `sigmoid`. By doing this, the resulting figures demonstrate how these different kernels affect the classification.

Note how the resulting decisions surfaces are no longer dominated by the linear divisions. Each of the last three decision surfaces have curved hyperplanes in the original space, since they transform the original data by using non-linear functions. Radial peforms the best.

#### Linear

```{r svmsetup2, echo=FALSE}

# construct larger random data set
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2.5
x[101:150,] <- x[101:150,] - 2.5
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")

library(e1071)

# set pseudorandom number generator
set.seed(123)
# sample training data and fit model
train <- base::sample(200,100, replace = FALSE)
```

```{r linear}

svmfit <- svm(y~., data = dat[train,],  kernel = "linear")
# plot classifier
plot(svmfit, dat)
```

#### Radial

```{r radial}

svmfit <- svm(y~., data = dat[train,], kernel = "radial")
# plot classifier
plot(svmfit, dat)
```

#### Sigmoid

```{r sigmoid}

svmfit <- svm(y~., data = dat[train,], kernel = "sigmoid")
# plot classifier
plot(svmfit, dat)
```

#### Polynomial

```{r polynomial}

svmfit <- svm(y~., data = dat[train,], kernel = "polynomial")
# plot classifier
plot(svmfit, dat)
```

# Exercise 2

1.  Look back at the SVM's run for the iris data. What where the hyperparameters chose for the Linear kernel, Radial kernel, and the Polynomial kernel? What are each of these hyperparameters?

## **Classification: Adult Data** {#classification-adult-data}

We now transition to a more complex data set, the adult data from the UCI machine learning repository. These data are fully documented online at the UCI website.

```{r adult1}
# install.packages("devtools")
# devtools::install_github("tyluRp/ucimlr")

adult<-ucimlr::adult


knitr::kable(head(adult,100))%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "100%",height="300px")

```

With the data now loaded into a DataFrame, we can move on to creating our training and testing data sets, and employing support vector classification.

```{r splitdata}
set.seed(1)
#lets split the data 60/40
#obtain stratified sample
adult<-na.omit(adult)

strat_sample <- adult %>%
                  group_by(income) %>%
                  sample_n(size=1000)



adult2<-strat_sample%>%
  mutate(income2=if_else(income==">50K","high","low"))

trainIndex <- createDataPartition(adult2$income2, p = .6, list = FALSE, times = 1)

#look at the first few
#head(trainIndex)

#grab the data
SVMTrain <- adult2[ trainIndex,]
SVMTest  <- adult2[-trainIndex,]

adult_SVM <- train(
  form = factor(income2) ~ age+fnlwgt+education_num+factor(occupation)+factor(race)+factor(sex)+capital_gain+capital_loss+hours_per_week,
  data = SVMTrain,
  #here we add classProbs because we want probs
  trControl = trainControl(method = "cv", number = 10,
                           classProbs =  TRUE),
  method = "svmLinear",
  preProcess = c("center", "scale"),
  tuneLength = 10)

adult_SVM

summary(adult_SVM)

svm_Pred<-predict(adult_SVM,SVMTest,type="prob")

knitr::kable(svm_Pred)%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "100%",height="300px")

svmtestpred<-cbind(svm_Pred,SVMTest)

svmtestpred<-svmtestpred%>%
  mutate(prediction=if_else(high>=.5,"high","low"))

confusionMatrix(factor(svmtestpred$prediction),factor(svmtestpred$income2))


```

## **Support Vector Classification: Unbalanced Classes**

So far, we have dealt with classification tasks where we have sufficient examples of all classes in our training data set. Sometimes, however, we are given data sets that are unbalanced, where one or more classes are underrepresented in the training data. In general, this can become very problematic, and can lead to [subtle biases](https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/) that might be difficult to find until it is too late.

A classic example where unbalanced classes can arise is in fraud detection. For a company to remain in business, fraud should be a rare event, ideally well below one percent. Imagine you have been given a set of transactions, and your task is to predict fraud. In this case you might have 9,900 negative examples, and only 100 positive examples. If we simply want to achieve the highest performance model, we can always predict **no fraud** and our model will be accurate 99% of the time! Clearly this is not appropriate.

This naive approach, known as the zero model, is to always predict the class with the most training labels. While uninformative as a model, it can provide a useful baseline for performance. .

```{r unbalanced}

Original <- train(
  form = factor(income2) ~ age+fnlwgt+education_num+factor(occupation)+factor(race)+factor(sex)+
    capital_gain+capital_loss+hours_per_week,
  data = SVMTrain,
  #add roc for AUC
  metric = "ROC",
  #here we add classProbs because we want probs
  trControl = trainControl(method = "cv", number = 10,
                           classProbs =  TRUE,
                           summaryFunction = twoClassSummary),
  method = "svmRadial",
  preProcess = c("center", "scale"),
  tuneLength = 10)


down_inside<-train(
  form = factor(income2) ~ age+fnlwgt+education_num+factor(occupation)+factor(race)+factor(sex)+
    capital_gain+capital_loss+hours_per_week,
  data = SVMTrain,
  #add roc for AUC
  metric = "ROC",
  #here we add classProbs because we want probs
  trControl = trainControl(method = "cv", number = 10,
                           classProbs =  TRUE,
                           summaryFunction = twoClassSummary,
                           sampling = "down"),
  method = "svmRadial",
  preProcess = c("center", "scale"),
  tuneLength = 10)

up_inside<-train(
  form = factor(income2) ~ age+fnlwgt+education_num+factor(occupation)+factor(race)+factor(sex)+
    capital_gain+capital_loss+hours_per_week,
  data = SVMTrain,
  #add roc for AUC
  metric = "ROC",
  #here we add classProbs because we want probs
  trControl = trainControl(method = "cv", number = 10,
                           classProbs =  TRUE,
                           summaryFunction = twoClassSummary,
                           sampling = "up"),
  method = "svmRadial",
  preProcess = c("center", "scale"),
  tuneLength = 10)

smote_inside<-train(
  form = factor(income2) ~ age+fnlwgt+education_num+factor(occupation)+factor(race)+factor(sex)+
    capital_gain+capital_loss+hours_per_week,
  data = SVMTrain,
  #add roc for AUC
  metric = "ROC",
  #here we add classProbs because we want probs
  trControl = trainControl(method = "cv", number = 10,
                           classProbs =  TRUE,
                           summaryFunction = twoClassSummary,
                           sampling = "smote"),
  method = "svmRadial",
  preProcess = c("center", "scale"),
  tuneLength = 10)

inside_models <- list(original = Original,
                      down = down_inside,
                      up = up_inside,
                      SMOTE = smote_inside)

inside_resampling <- resamples(inside_models)

summary(inside_resampling, metric = "ROC")


test_roc <- function(model, data) {
  library(pROC)
  roc_obj <- roc(data$income2, 
                 predict(model, data, type = "prob")[, "high"],
                 levels = c("low", "high"))
  ci(roc_obj)
  }

inside_test <- lapply(inside_models, test_roc, data = SVMTest)
inside_test <- lapply(inside_test, as.vector)
inside_test <- do.call("rbind", inside_test)
colnames(inside_test) <- c("lower", "ROC", "upper")
inside_test <- as.data.frame(inside_test)

knitr::kable(inside_test)


```

## 

### **The ROC Curve and AUC**

While the standard performance metrics are useful for a standard classification task, many algorithms now generate a probabilistic classification. As a result, we need a method to not only compare different estimators, but determine the optimal threshold for an estimator. To support this decision, we employ the [receiver operating characteristic (ROC) curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). Originally developed during World War II to predict the performance of an individual using a radar system, the ROC curve displays the relationship between the number of false positives (along the x-axis) and true positives (along the y-axis) as a function of probability threshold.

The ROC curve starts at the lower left, where nothing has been classified. From here the estimator is used to determine the true and false positives for very high probability thresholds. At this point, the curve should shoot upward from the lower left, wince we expect a good classifier (and what other type of classifier would we build) performs well at high threshold. In general, as the probability threshold is lowered, we will begin to predict more false positives, and thus the curve will shift to the right.

To generate a ROC curve, we need to create arrays of false and true positives at different probability thresholds. Given an ROC curve, another performance metric that can be measured is the *area under the curve* or AUC. In an ideal case this metric has the value of one, or perfect classification, and a random classification has the value of 0.5. This metric can provide a useful comparison between different estimators on the same data.

```{r roc1}
origroc<-pROC::roc(SVMTest$income2, 
                 predict(Original, SVMTest, type = "prob")[, "high"],
                 levels = c("low", "high"))

downroc<-pROC::roc(SVMTest$income2, 
                 predict(down_inside, SVMTest, type = "prob")[, "high"],
                 levels = c("low", "high"))

uproc<-pROC::roc(SVMTest$income2, 
                 predict(up_inside, SVMTest, type = "prob")[, "high"],
                 levels = c("low", "high"))

smoteroc<-pROC::roc(SVMTest$income2, 
                 predict(smote_inside, SVMTest, type = "prob")[, "high"],
                 levels = c("low", "high"))



pROC::ggroc(list(origroc,downroc,uproc,smoteroc))
```

When interpreting a ROC curve, the goal is to approximate as closely as possible to the perfect classifier, which reaches 100% true positive at zero false positive. On the other hand, an estimator should perform better than the baseline, which is essentially a random guess.

### **The Gain and Lift Charts**

#### Gain

While the ROC curve provides a lot of detail into the performance of an estimator, and allows the performance of different estimators to be compared, we will sometimes want a different performance metric. For example, we may not have a sufficient budget to target all customers on which we have data. Thus, we can build a classifier to estimate which of our customers makes more than fifty thousand dollars a year. However, even this subset might be too large. What we really want is a way to select those instances from our classified data that we believe have the highest likelihood of being in our target category. Two charts that are useful to accomplish this goal are the *gain chart* and the related [*lift chart*](https://en.wikipedia.org/wiki/Lift_(data_mining)).

A classic example of where a lift chart is often used is in marketing. If we have a limited budget, we want to optimally target customers who will respond positively to ads or a marketing campaign. As a result, we would build a model and compute lift chart. This chart can be used to infer a cutoff point in the sample, above which we have the highest likelihood of a positive response. This will allow us to target customers optimally given a limited budget.

An alternative example is preventing customer churn, which is where customers migrate from one company to another. We can use a classifier to predict how likely a customer is to churn. With a lift chart, we can identify those customers most likely to churn and focus our limited retention budget on keeping them with our company.

```{r gain}

pred<-ROCR::prediction(svmtestpred$high ,factor(svmtestpred$income2))

gain <- ROCR::performance(pred, "tpr", "rpp")

plot(gain, main = "Gain Chart")
```

#### Lift

In the following Code chunk, we first compute the lift curve for our three different classification estimators. In this case, the lift is simply the gain for the estimator divided by the baseline response. To [interpret the lift curve](http://blog.datalifebalance.com/lift-charts-a-data-scientists-secret-weapon/), recall that the value of the lift curve at any point indicates the relative improvement of our model over random.

From this curve, we can see that our default logistic regression and support vector classification algorithms both perform very well. But more importantly, if this classification was being used to target customers, we can optimize our results and use less money by targeting those customers who fall to the left in this chart. In some cases, you may see these curves converted into a profit curve, where the value of the prediction is included. In this example, we might assume a cost per targeted ad to convert the lift curve into a cost curve, which would allow us to determine how many (and who) of the individuals we should target given a budget. However, these types of curves are generally domain specific, so we do not present them in this notebook.

```{r lift}


pred<-ROCR::prediction(svmtestpred$high ,factor(svmtestpred$income2))

perf <- ROCR::performance(pred,"lift","rpp")

plot(perf, main="Lift curve", colorize=T) 
```

[Another explanation](https://www.statistics.com/word-of-the-week-roc-lift-and-gains-curves/)

Don't be afraid to search for better explanations.

## **Support Vector Machine: Regression**

To this point, we have only applied the support vector machine algorithm to classification tasks. This algorithm can also be applied to regression tasks by using hyperplanes to model the data distribution. Basically, this algorithm works on regression problems by mapping the training data into a high dimensional space, which might involve a non-linear kernel, and performing something akin to linear regression on the data in this higher dimensional space. Recall that for best performance, SVMs require the data be normalized prior to use.

```{r svmreg}

library(caret)
library(tidyverse)
#set the seed :)
set.seed(1)
#get our samples
#using the iris data
#lets split the data 60/40

trainIndex <- createDataPartition(iris$Sepal.Width, p = .6, list = FALSE, times = 1)

#look at the first few
#head(trainIndex)

#grab the data
SVMTrain <- iris[ trainIndex,]
SVMTest  <- iris[-trainIndex,]


iris_SVM <- train(
  form = Sepal.Width ~ Sepal.Length+Petal.Width+Petal.Length,
  data = SVMTrain,
  trControl = trainControl(method = "cv", number = 10),
  method = "svmRadial",
  preProcess = c("center", "scale"),
  tuneLength = 10)

iris_SVM

summary(iris_SVM)

svm_Pred<-predict(iris_SVM,SVMTest)

knitr::kable(svm_Pred)%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "100%",height="300px")

svmtestpred<-cbind(svm_Pred,SVMTest)
#root mean squared error
RMSE(svmtestpred$svm_Pred,svmtestpred$Sepal.Width)
#best measure ever...RSquared 
cor(svmtestpred$svm_Pred,svmtestpred$Sepal.Width)^2
```

👆😆

# Exercise 3

1.  Run a regression using the adult data from the [Classification: Adult Data](#classification-adult-data) section. Note: be sure to sample the data, i.e. use adult2.

fin
