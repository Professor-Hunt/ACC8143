{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\nThe is a course website for ACC 8143 Accounting Data Analytics. This course is derived from a Deloitte Data Analytics course made by the University of Illinois.\r\nStudents will use R statistical software for ETL (extraction, transformation, and loading data). Specifically, R and RStudio (GUI for R) will be used in conjunction with Tidyverse R packages for advanced manipulation and visualizations. Students will learn to use Git and GitHub. Git and GitHub are extensively used for data analytics in practice. GitHub will show the students the importance of version control and the value of coding with care. Students will also learn Markdown which is a framework for presenting interactive reports, dashboards, and creating websites. Students will use RStudio, GitHub, Markdown, and Distill r package to make a professional website for showcasing their work in the class. This will help students learn the value of presenting information in a business analytics context and presenting their own work for others to see. Using Rstudio the students will learn how popular machine learning algorithms work, the difference between parametric and nonparametric algorithms, supervised and unsupervised algorithms, the bias-variance trade-off, and overfitting and underfitting algorithms with hands on examples in business contexts.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-09-28T13:39:03-05:00"
    },
    {
      "path": "index.html",
      "title": "ACC8143",
      "description": "Accounting Data Analytics\n",
      "author": [
        {
          "name": "Joshua O.S. Hunt",
          "url": "https://professor-hunt.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\r\nThis Website will continue to evolve. I am running my course through our LMS (canvas), but I am providing this website and source code for other professors and students to use.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-18T10:37:27-05:00"
    },
    {
      "path": "Module_1.html",
      "title": "Module 1",
      "description": "Introduction to machine learning\n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nGame Plan\r\nObjectives\r\nBy the end of this module, you should be able to:\r\nArtificial Intelligence and Accountancy\r\nInteresting Articles\r\n\r\nIntroduction to Machine Learning\r\nThe Four Types of Data Analysis are:\r\n1. Descriptive Analytics: What is happening?\r\n2. Diagnostic Analytics: Why is it happening?\r\n3. Predictive Analytics: What is likely to happen?\r\n4. Prescriptive Analytics: What do I need to do?\r\n\r\nGetting our hands dirty\r\nSet up\r\nSide Note‚Ä¶Other plots üôà\r\n\r\n\r\nIntroducing Machine Learning\r\nIntroducing Caret\r\nData Pre-Processing\r\nData Scaling\r\nClassification\r\nRegression\r\nRsquared üòà\r\nDimensionality Reduction\r\n\r\n\r\nExercise 1\r\nModel Persistence\r\nDeep Dive\r\n\r\nRegression\r\nIntroduction to Linear Regression\r\nObjectives\r\nLinear Regression\r\n\r\nRecap\r\n\r\n\r\nGame Plan\r\nThis module provides the basis for the rest of the course by introducing the basic concepts behind machine learning, and, specifically, how to perform machine learning by using RStudio and the CARET R package. First, you will learn how machine learning and artificial intelligence are disrupting businesses. Next, you will learn about the basic types of machine learning and how to leverage these algorithms in a R script. Third, you will learn how linear regression can be considered a machine learning problem with parameters that must be determined computationally by minimizing a cost function. Finally, you will learn about neighbor-based algorithms, including the k-nearest neighbor algorithm, which can be used for both classification and regression tasks.\r\nObjectives\r\nBy the end of this module, you should be able to:\r\narticulate the different types of machine learning algorithms and provide examples where each type might be applied within the Accounting profession,\r\ndescribe the challenges in cleaning and pre-processing data,\r\napply the Caret Package to perform basic machine learning tasks,\r\nunderstand the importance of a cost function and the importance of minimizing this functions,\r\nperform general linear regression by using the Caret Package, and\r\napply the k-nearest neighbor algorithm for both classification and regression tasks.\r\nArtificial Intelligence and Accountancy\r\nThis lesson explores the fundamentals of machine learning and artificial intelligence and how these tools are being used in accountancy and business in general.\r\nInteresting Articles\r\nHow is accountancy and finance world using artificial intelligence\r\nFinancial Statement Audits\r\nWhat is the growing role artificial intelligence (and by association, machine learning) play in modern accounting?\r\nWhat does the impact technology plays in shaping careers in modern accountancy?\r\nHow is artificial intelligence impacting financial auditing?\r\nIntroduction to Machine Learning1\r\n\r\nYour most important skill will be your ability to translate data into insights that are clear and meaningful to a stakeholder.\r\nThe Four Types of Data Analysis are:\r\n1. Descriptive Analytics: What is happening?\r\nThis is the most common of all forms. In business, it provides the analyst with a view of key metrics and measures within the company.\r\nAn example of this could be a monthly profit and loss statement. Similarly, an analyst could have data on a large population of customers. Understanding demographic information on their customers (e.g.¬†30% of our customers are self-employed) would be categorised as ‚Äúdescriptive analytics‚Äù. Utilising useful visualisation tools enhances the message of descriptive analytics.\r\n2. Diagnostic Analytics: Why is it happening?\r\nThis is the next step in complexity in data analytics is descriptive analytics. On the assessment of the descriptive data, diagnostic analytical tools will empower an analyst to drill down and in so doing isolate the root-cause of a problem.\r\nWell-designed business information (BI) dashboards incorporating reading of time-series data (i.e.¬†data over multiple successive points in time) and featuring filters and drill down capability allow for such analysis.\r\n3. Predictive Analytics: What is likely to happen?\r\nPredictive analytics is all about forecasting. Whether it‚Äôs the likelihood of an event happening in future, forecasting a quantifiable amount or estimating a point in time at which something might happen - these are all done through predictive models.\r\nPredictive models typically utilise a variety of variable data to make the prediction. The variability of the component data will have a relationship with what it is likely to predict (e.g.¬†the older a person, the more susceptible they are to a heart-attack ‚Äì we would say that age has a linear correlation with heart-attack risk). These data are then compiled together into a score or prediction.\r\nIn a world of significant uncertainty, being able to predict allows one to make better decisions. Predictive models are some of the most important utilised across many fields.\r\nCommon Issues with Prediction\r\n4. Prescriptive Analytics: What do I need to do?\r\nThe next step up regarding value and complexity is the prescriptive model.¬† The prescriptive model utilises an understanding of what has happened, why it has happened and a variety of ‚Äúwhat-might-happen‚Äù analysis to help the user determine the best course of action to take. A prescriptive analysis is typically not just with one individual response but is, in fact, a host of other actions.\r\nAn excellent example of this is a traffic application helping you choose the best route home and taking into account the distance of each route, the speed at which one can travel on each road and, crucially, the current traffic constraints.\r\nAnother example might be producing an exam time-table such that no students have clashing schedules.\r\nGetting our hands dirty\r\nAs a field, machine learning is both expansive and mathematically complex. From deriving simple linear relationships via regression analysis to finding clusters of data points in an N-dimensional space; statistical and machine learning techniques can take years to fully master. However, given the short time available in this course, we will take the simpler approach of demonstrating several commonly used approaches in order to both introduce the fundamental concepts in machine learning and the methodology we will use in RStudio to apply these concepts to actual data. For the latter, we will use the standard machine learning library in R, which is the Caret Package.\r\nWe will demonstrate the four main tasks of machine learning: classification, regression, dimensional reduction, and clustering. Note that this module is simply an introduction to these topics, we will explore these and other areas in more detail throughout this course. Finally, we will discuss how to persist machine learning models.\r\nSet up\r\nMake sure you have the caret package installed.\r\n\r\n\r\ninstall.packages(\"caret\")\r\n\r\n\r\n\r\nRun your libraries\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(caret)\r\n\r\n\r\n\r\nThe first steps in any data analytics effort, once the business goal has been defined, are to understand and prepare the data of interest. For example, the Cross Industry Standard Process for Data Mining or (CRISP-DM) starts with the Business Understanding step, immediately followed by the Data Understanding and Data Preparation steps. For machine learning analyses, these latter two steps require loading the data into our notebook, exploring the data either systematically or in a cumulative sense to understand the typical features for different instances. We also can generate descriptive statistical summaries and visualizations, such as a pair plot, to understand the data in full. Finally, we will need to clean the data to properly account for missing data, data that are incomplete or formatted incorrectly, or to generate meta-features (such as a date-time) from existing features.\r\n\r\nFor this module, we will focus on a single, simple data set, the standard Iris dataset, which is included by default. Note that given a data set, such as the Iris data, we have rows, which correspond to different instances (e.g., different flowers), and columns, which correspond to different features of the instances (e.g., different measurements of the flowers). To understand the data, we first load this data into RStudio, before looking at several instances from the data. Next, we will group the data by species to explore cumulative quantities, before extracting a statistical summary of the entire data set. Finally, we will generate a pair plot to visually explore the data. Since this data has already been cleaned (and only consists of four features) we will not need to perform additional tasks.\r\nLoad the data\r\n\r\n\r\n#load the data\r\niris<-iris\r\n\r\n\r\n\r\nThe data set consists of 150 total measurements of three different types of Iris flowers, equally divided between three classes: Iris setosa, Iris versicolor, and Iris virginica. Before proceeding, we can examine the DataFrame that contains these data to view typical instances, to see a cumulative summary, and a brief statistical summary.\r\n\r\n\r\n#examine the top 5 rows\r\nhead(iris,5)\r\n\r\n\r\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n1          5.1         3.5          1.4         0.2  setosa\r\n2          4.9         3.0          1.4         0.2  setosa\r\n3          4.7         3.2          1.3         0.2  setosa\r\n4          4.6         3.1          1.5         0.2  setosa\r\n5          5.0         3.6          1.4         0.2  setosa\r\n\r\n#view the whole dataset\r\nknitr::kable(iris)%>%\r\n  kableExtra::kable_styling(\"striped\")%>%\r\n  kableExtra::scroll_box(width = \"100%\",height=\"300px\")\r\n\r\n\r\n\r\n\r\nSepal.Length\r\n\r\n\r\nSepal.Width\r\n\r\n\r\nPetal.Length\r\n\r\n\r\nPetal.Width\r\n\r\n\r\nSpecies\r\n\r\n\r\n5.1\r\n\r\n\r\n3.5\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.9\r\n\r\n\r\n3.0\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.7\r\n\r\n\r\n3.2\r\n\r\n\r\n1.3\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.6\r\n\r\n\r\n3.1\r\n\r\n\r\n1.5\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.0\r\n\r\n\r\n3.6\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.4\r\n\r\n\r\n3.9\r\n\r\n\r\n1.7\r\n\r\n\r\n0.4\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.6\r\n\r\n\r\n3.4\r\n\r\n\r\n1.4\r\n\r\n\r\n0.3\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.0\r\n\r\n\r\n3.4\r\n\r\n\r\n1.5\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.4\r\n\r\n\r\n2.9\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.9\r\n\r\n\r\n3.1\r\n\r\n\r\n1.5\r\n\r\n\r\n0.1\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.4\r\n\r\n\r\n3.7\r\n\r\n\r\n1.5\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.8\r\n\r\n\r\n3.4\r\n\r\n\r\n1.6\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.8\r\n\r\n\r\n3.0\r\n\r\n\r\n1.4\r\n\r\n\r\n0.1\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.3\r\n\r\n\r\n3.0\r\n\r\n\r\n1.1\r\n\r\n\r\n0.1\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.8\r\n\r\n\r\n4.0\r\n\r\n\r\n1.2\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.7\r\n\r\n\r\n4.4\r\n\r\n\r\n1.5\r\n\r\n\r\n0.4\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.4\r\n\r\n\r\n3.9\r\n\r\n\r\n1.3\r\n\r\n\r\n0.4\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.1\r\n\r\n\r\n3.5\r\n\r\n\r\n1.4\r\n\r\n\r\n0.3\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.7\r\n\r\n\r\n3.8\r\n\r\n\r\n1.7\r\n\r\n\r\n0.3\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.1\r\n\r\n\r\n3.8\r\n\r\n\r\n1.5\r\n\r\n\r\n0.3\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.4\r\n\r\n\r\n3.4\r\n\r\n\r\n1.7\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.1\r\n\r\n\r\n3.7\r\n\r\n\r\n1.5\r\n\r\n\r\n0.4\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.6\r\n\r\n\r\n3.6\r\n\r\n\r\n1.0\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.1\r\n\r\n\r\n3.3\r\n\r\n\r\n1.7\r\n\r\n\r\n0.5\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.8\r\n\r\n\r\n3.4\r\n\r\n\r\n1.9\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.0\r\n\r\n\r\n3.0\r\n\r\n\r\n1.6\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.0\r\n\r\n\r\n3.4\r\n\r\n\r\n1.6\r\n\r\n\r\n0.4\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.2\r\n\r\n\r\n3.5\r\n\r\n\r\n1.5\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.2\r\n\r\n\r\n3.4\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.7\r\n\r\n\r\n3.2\r\n\r\n\r\n1.6\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.8\r\n\r\n\r\n3.1\r\n\r\n\r\n1.6\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.4\r\n\r\n\r\n3.4\r\n\r\n\r\n1.5\r\n\r\n\r\n0.4\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.2\r\n\r\n\r\n4.1\r\n\r\n\r\n1.5\r\n\r\n\r\n0.1\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.5\r\n\r\n\r\n4.2\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.9\r\n\r\n\r\n3.1\r\n\r\n\r\n1.5\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.0\r\n\r\n\r\n3.2\r\n\r\n\r\n1.2\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.5\r\n\r\n\r\n3.5\r\n\r\n\r\n1.3\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.9\r\n\r\n\r\n3.6\r\n\r\n\r\n1.4\r\n\r\n\r\n0.1\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.4\r\n\r\n\r\n3.0\r\n\r\n\r\n1.3\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.1\r\n\r\n\r\n3.4\r\n\r\n\r\n1.5\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.0\r\n\r\n\r\n3.5\r\n\r\n\r\n1.3\r\n\r\n\r\n0.3\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.5\r\n\r\n\r\n2.3\r\n\r\n\r\n1.3\r\n\r\n\r\n0.3\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.4\r\n\r\n\r\n3.2\r\n\r\n\r\n1.3\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.0\r\n\r\n\r\n3.5\r\n\r\n\r\n1.6\r\n\r\n\r\n0.6\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.1\r\n\r\n\r\n3.8\r\n\r\n\r\n1.9\r\n\r\n\r\n0.4\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.8\r\n\r\n\r\n3.0\r\n\r\n\r\n1.4\r\n\r\n\r\n0.3\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.1\r\n\r\n\r\n3.8\r\n\r\n\r\n1.6\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n4.6\r\n\r\n\r\n3.2\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.3\r\n\r\n\r\n3.7\r\n\r\n\r\n1.5\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n5.0\r\n\r\n\r\n3.3\r\n\r\n\r\n1.4\r\n\r\n\r\n0.2\r\n\r\n\r\nsetosa\r\n\r\n\r\n7.0\r\n\r\n\r\n3.2\r\n\r\n\r\n4.7\r\n\r\n\r\n1.4\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.4\r\n\r\n\r\n3.2\r\n\r\n\r\n4.5\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.9\r\n\r\n\r\n3.1\r\n\r\n\r\n4.9\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.5\r\n\r\n\r\n2.3\r\n\r\n\r\n4.0\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.5\r\n\r\n\r\n2.8\r\n\r\n\r\n4.6\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.7\r\n\r\n\r\n2.8\r\n\r\n\r\n4.5\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.3\r\n\r\n\r\n3.3\r\n\r\n\r\n4.7\r\n\r\n\r\n1.6\r\n\r\n\r\nversicolor\r\n\r\n\r\n4.9\r\n\r\n\r\n2.4\r\n\r\n\r\n3.3\r\n\r\n\r\n1.0\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.6\r\n\r\n\r\n2.9\r\n\r\n\r\n4.6\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.2\r\n\r\n\r\n2.7\r\n\r\n\r\n3.9\r\n\r\n\r\n1.4\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.0\r\n\r\n\r\n2.0\r\n\r\n\r\n3.5\r\n\r\n\r\n1.0\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.9\r\n\r\n\r\n3.0\r\n\r\n\r\n4.2\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.0\r\n\r\n\r\n2.2\r\n\r\n\r\n4.0\r\n\r\n\r\n1.0\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.1\r\n\r\n\r\n2.9\r\n\r\n\r\n4.7\r\n\r\n\r\n1.4\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.6\r\n\r\n\r\n2.9\r\n\r\n\r\n3.6\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.7\r\n\r\n\r\n3.1\r\n\r\n\r\n4.4\r\n\r\n\r\n1.4\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.6\r\n\r\n\r\n3.0\r\n\r\n\r\n4.5\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.8\r\n\r\n\r\n2.7\r\n\r\n\r\n4.1\r\n\r\n\r\n1.0\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.2\r\n\r\n\r\n2.2\r\n\r\n\r\n4.5\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.6\r\n\r\n\r\n2.5\r\n\r\n\r\n3.9\r\n\r\n\r\n1.1\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.9\r\n\r\n\r\n3.2\r\n\r\n\r\n4.8\r\n\r\n\r\n1.8\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.1\r\n\r\n\r\n2.8\r\n\r\n\r\n4.0\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.3\r\n\r\n\r\n2.5\r\n\r\n\r\n4.9\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.1\r\n\r\n\r\n2.8\r\n\r\n\r\n4.7\r\n\r\n\r\n1.2\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.4\r\n\r\n\r\n2.9\r\n\r\n\r\n4.3\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.6\r\n\r\n\r\n3.0\r\n\r\n\r\n4.4\r\n\r\n\r\n1.4\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.8\r\n\r\n\r\n2.8\r\n\r\n\r\n4.8\r\n\r\n\r\n1.4\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.7\r\n\r\n\r\n3.0\r\n\r\n\r\n5.0\r\n\r\n\r\n1.7\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.0\r\n\r\n\r\n2.9\r\n\r\n\r\n4.5\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.7\r\n\r\n\r\n2.6\r\n\r\n\r\n3.5\r\n\r\n\r\n1.0\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.5\r\n\r\n\r\n2.4\r\n\r\n\r\n3.8\r\n\r\n\r\n1.1\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.5\r\n\r\n\r\n2.4\r\n\r\n\r\n3.7\r\n\r\n\r\n1.0\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.8\r\n\r\n\r\n2.7\r\n\r\n\r\n3.9\r\n\r\n\r\n1.2\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.0\r\n\r\n\r\n2.7\r\n\r\n\r\n5.1\r\n\r\n\r\n1.6\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.4\r\n\r\n\r\n3.0\r\n\r\n\r\n4.5\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.0\r\n\r\n\r\n3.4\r\n\r\n\r\n4.5\r\n\r\n\r\n1.6\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.7\r\n\r\n\r\n3.1\r\n\r\n\r\n4.7\r\n\r\n\r\n1.5\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.3\r\n\r\n\r\n2.3\r\n\r\n\r\n4.4\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.6\r\n\r\n\r\n3.0\r\n\r\n\r\n4.1\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.5\r\n\r\n\r\n2.5\r\n\r\n\r\n4.0\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.5\r\n\r\n\r\n2.6\r\n\r\n\r\n4.4\r\n\r\n\r\n1.2\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.1\r\n\r\n\r\n3.0\r\n\r\n\r\n4.6\r\n\r\n\r\n1.4\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.8\r\n\r\n\r\n2.6\r\n\r\n\r\n4.0\r\n\r\n\r\n1.2\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.0\r\n\r\n\r\n2.3\r\n\r\n\r\n3.3\r\n\r\n\r\n1.0\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.6\r\n\r\n\r\n2.7\r\n\r\n\r\n4.2\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.7\r\n\r\n\r\n3.0\r\n\r\n\r\n4.2\r\n\r\n\r\n1.2\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.7\r\n\r\n\r\n2.9\r\n\r\n\r\n4.2\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.2\r\n\r\n\r\n2.9\r\n\r\n\r\n4.3\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.1\r\n\r\n\r\n2.5\r\n\r\n\r\n3.0\r\n\r\n\r\n1.1\r\n\r\n\r\nversicolor\r\n\r\n\r\n5.7\r\n\r\n\r\n2.8\r\n\r\n\r\n4.1\r\n\r\n\r\n1.3\r\n\r\n\r\nversicolor\r\n\r\n\r\n6.3\r\n\r\n\r\n3.3\r\n\r\n\r\n6.0\r\n\r\n\r\n2.5\r\n\r\n\r\nvirginica\r\n\r\n\r\n5.8\r\n\r\n\r\n2.7\r\n\r\n\r\n5.1\r\n\r\n\r\n1.9\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.1\r\n\r\n\r\n3.0\r\n\r\n\r\n5.9\r\n\r\n\r\n2.1\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.3\r\n\r\n\r\n2.9\r\n\r\n\r\n5.6\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.5\r\n\r\n\r\n3.0\r\n\r\n\r\n5.8\r\n\r\n\r\n2.2\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.6\r\n\r\n\r\n3.0\r\n\r\n\r\n6.6\r\n\r\n\r\n2.1\r\n\r\n\r\nvirginica\r\n\r\n\r\n4.9\r\n\r\n\r\n2.5\r\n\r\n\r\n4.5\r\n\r\n\r\n1.7\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.3\r\n\r\n\r\n2.9\r\n\r\n\r\n6.3\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.7\r\n\r\n\r\n2.5\r\n\r\n\r\n5.8\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.2\r\n\r\n\r\n3.6\r\n\r\n\r\n6.1\r\n\r\n\r\n2.5\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.5\r\n\r\n\r\n3.2\r\n\r\n\r\n5.1\r\n\r\n\r\n2.0\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.4\r\n\r\n\r\n2.7\r\n\r\n\r\n5.3\r\n\r\n\r\n1.9\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.8\r\n\r\n\r\n3.0\r\n\r\n\r\n5.5\r\n\r\n\r\n2.1\r\n\r\n\r\nvirginica\r\n\r\n\r\n5.7\r\n\r\n\r\n2.5\r\n\r\n\r\n5.0\r\n\r\n\r\n2.0\r\n\r\n\r\nvirginica\r\n\r\n\r\n5.8\r\n\r\n\r\n2.8\r\n\r\n\r\n5.1\r\n\r\n\r\n2.4\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.4\r\n\r\n\r\n3.2\r\n\r\n\r\n5.3\r\n\r\n\r\n2.3\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.5\r\n\r\n\r\n3.0\r\n\r\n\r\n5.5\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.7\r\n\r\n\r\n3.8\r\n\r\n\r\n6.7\r\n\r\n\r\n2.2\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.7\r\n\r\n\r\n2.6\r\n\r\n\r\n6.9\r\n\r\n\r\n2.3\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.0\r\n\r\n\r\n2.2\r\n\r\n\r\n5.0\r\n\r\n\r\n1.5\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.9\r\n\r\n\r\n3.2\r\n\r\n\r\n5.7\r\n\r\n\r\n2.3\r\n\r\n\r\nvirginica\r\n\r\n\r\n5.6\r\n\r\n\r\n2.8\r\n\r\n\r\n4.9\r\n\r\n\r\n2.0\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.7\r\n\r\n\r\n2.8\r\n\r\n\r\n6.7\r\n\r\n\r\n2.0\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.3\r\n\r\n\r\n2.7\r\n\r\n\r\n4.9\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.7\r\n\r\n\r\n3.3\r\n\r\n\r\n5.7\r\n\r\n\r\n2.1\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.2\r\n\r\n\r\n3.2\r\n\r\n\r\n6.0\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.2\r\n\r\n\r\n2.8\r\n\r\n\r\n4.8\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.1\r\n\r\n\r\n3.0\r\n\r\n\r\n4.9\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.4\r\n\r\n\r\n2.8\r\n\r\n\r\n5.6\r\n\r\n\r\n2.1\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.2\r\n\r\n\r\n3.0\r\n\r\n\r\n5.8\r\n\r\n\r\n1.6\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.4\r\n\r\n\r\n2.8\r\n\r\n\r\n6.1\r\n\r\n\r\n1.9\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.9\r\n\r\n\r\n3.8\r\n\r\n\r\n6.4\r\n\r\n\r\n2.0\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.4\r\n\r\n\r\n2.8\r\n\r\n\r\n5.6\r\n\r\n\r\n2.2\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.3\r\n\r\n\r\n2.8\r\n\r\n\r\n5.1\r\n\r\n\r\n1.5\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.1\r\n\r\n\r\n2.6\r\n\r\n\r\n5.6\r\n\r\n\r\n1.4\r\n\r\n\r\nvirginica\r\n\r\n\r\n7.7\r\n\r\n\r\n3.0\r\n\r\n\r\n6.1\r\n\r\n\r\n2.3\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.3\r\n\r\n\r\n3.4\r\n\r\n\r\n5.6\r\n\r\n\r\n2.4\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.4\r\n\r\n\r\n3.1\r\n\r\n\r\n5.5\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.0\r\n\r\n\r\n3.0\r\n\r\n\r\n4.8\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.9\r\n\r\n\r\n3.1\r\n\r\n\r\n5.4\r\n\r\n\r\n2.1\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.7\r\n\r\n\r\n3.1\r\n\r\n\r\n5.6\r\n\r\n\r\n2.4\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.9\r\n\r\n\r\n3.1\r\n\r\n\r\n5.1\r\n\r\n\r\n2.3\r\n\r\n\r\nvirginica\r\n\r\n\r\n5.8\r\n\r\n\r\n2.7\r\n\r\n\r\n5.1\r\n\r\n\r\n1.9\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.8\r\n\r\n\r\n3.2\r\n\r\n\r\n5.9\r\n\r\n\r\n2.3\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.7\r\n\r\n\r\n3.3\r\n\r\n\r\n5.7\r\n\r\n\r\n2.5\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.7\r\n\r\n\r\n3.0\r\n\r\n\r\n5.2\r\n\r\n\r\n2.3\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.3\r\n\r\n\r\n2.5\r\n\r\n\r\n5.0\r\n\r\n\r\n1.9\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.5\r\n\r\n\r\n3.0\r\n\r\n\r\n5.2\r\n\r\n\r\n2.0\r\n\r\n\r\nvirginica\r\n\r\n\r\n6.2\r\n\r\n\r\n3.4\r\n\r\n\r\n5.4\r\n\r\n\r\n2.3\r\n\r\n\r\nvirginica\r\n\r\n\r\n5.9\r\n\r\n\r\n3.0\r\n\r\n\r\n5.1\r\n\r\n\r\n1.8\r\n\r\n\r\nvirginica\r\n\r\n\r\n\r\n\r\n\r\n#examine grouped data\r\niris%>%\r\n  group_by(Species)%>%\r\n  summarise(count=n())\r\n\r\n\r\n# A tibble: 3 x 2\r\n  Species    count\r\n  <fct>      <int>\r\n1 setosa        50\r\n2 versicolor    50\r\n3 virginica     50\r\n\r\n\r\n\r\n# Get descriptive statistics\r\nsummary(iris)\r\n\r\n\r\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \r\n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \r\n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \r\n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \r\n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \r\n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \r\n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \r\n       Species  \r\n setosa    :50  \r\n versicolor:50  \r\n virginica :50  \r\n                \r\n                \r\n                \r\n\r\nAnother handy package\r\n\r\n\r\ninstall.packages(\"psych\")\r\n\r\n\r\n\r\n\r\n\r\npsych::describe(iris)\r\n\r\n\r\n             vars   n mean   sd median trimmed  mad min max range\r\nSepal.Length    1 150 5.84 0.83   5.80    5.81 1.04 4.3 7.9   3.6\r\nSepal.Width     2 150 3.06 0.44   3.00    3.04 0.44 2.0 4.4   2.4\r\nPetal.Length    3 150 3.76 1.77   4.35    3.76 1.85 1.0 6.9   5.9\r\nPetal.Width     4 150 1.20 0.76   1.30    1.18 1.04 0.1 2.5   2.4\r\nSpecies*        5 150 2.00 0.82   2.00    2.00 1.48 1.0 3.0   2.0\r\n              skew kurtosis   se\r\nSepal.Length  0.31    -0.61 0.07\r\nSepal.Width   0.31     0.14 0.04\r\nPetal.Length -0.27    -1.42 0.14\r\nPetal.Width  -0.10    -1.36 0.06\r\nSpecies*      0.00    -1.52 0.07\r\n\r\n# ?psych::describe\r\n\r\n\r\n\r\nLook up what mad, se, skewness, and kurtosis is‚Ä¶ üïµÔ∏è\r\nAs demonstrated by the output from the previous code cells, our test data matches our expectations (note that the full Iris data set is listed on Wikipedia). These data consist of three types, each with fifty instances, and every row has four measured features (i.e., attributes). The four primary features of the data are Sepal Length, Sepal Width, Petal Length, and Petal Width. In simple terms, petals are the showy, colorful part of the Iris flower, while the sepals provide protection and support for the petals.\r\nIn addition, our cursory exploration of the DataFrame indicated the data are clean. One simple way to verify this is that the count is the same for every feature, and the descriptive statistics (e.g., min, max, and mean) are all numerical. If we had missing or bad data in our DataFrame, these measures would generally indicate the problem. If there were missing data, we could drop any instance with missing data by using the na.omit method, or alternatively insert a value by using mutate . An alternative, and powerful, technique for handling missing data is known as imputing, where we apply machine learning2 to generate realistic values for any missing data. This approach will be demonstrated in a subsequent module.\r\nAt this point, we have loaded our data, and verified the data are clean. The next step is to visualize the relationships between the different features in our data.\r\nLets use another package üòÜ\r\n\r\n\r\ninstall.packages(\"GGally\")\r\n\r\n\r\n\r\nLook at me üëÄ ggpairs function üëÄ\r\n\r\n\r\nGGally::ggpairs(iris)\r\n\r\n\r\n\r\n\r\n\r\n\r\nGGally::ggpairs(iris, aes(color = Species, alpha = 0.5))\r\n\r\n\r\n\r\n\r\nThese figures indicate that the three Iris species cluster naturally in these dimensions, with minimal overlap. As a result, these data provide an excellent test for different machine learning algorithms.\r\nFirst, however, we will generate one scatter plot that displays a larger version of the Sepal Width versus Petal Width scatter plot to highlight the inherent structure in these data. Furthermore, we will refer back to this plot in later analyses in this Module.\r\n\r\n\r\n#crash course in plots?\r\nggplot(data=iris,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Species))+geom_point(alpha=0.5)\r\n\r\n\r\n\r\n\r\nSide Note‚Ä¶Other plots üôà\r\n\r\nLine Graph\r\n\r\n\r\n#line graph..this is a TERRIBLE example...\r\n#ask me why\r\niris.setosa<-iris%>%\r\n  filter(Species==\"setosa\")%>%\r\n  distinct(Sepal.Width,.keep_all = TRUE)\r\n\r\nggplot(data=iris.setosa,mapping = aes(x=Sepal.Width,y=Sepal.Length))+geom_line()\r\n\r\n\r\n\r\n\r\nHistograms\r\n\r\n\r\nggplot(data=iris,mapping = aes(x=Sepal.Length,color=Species))+geom_histogram()\r\n\r\n\r\n\r\n\r\nBoxplots\r\n\r\n\r\nggplot(data=iris,mapping = aes(x=Species,y=Sepal.Length))+geom_boxplot()\r\n\r\n\r\n\r\n\r\nBar plots\r\n\r\n\r\nggplot(data=iris,mapping = aes(x=Species))+geom_bar()\r\n\r\n\r\n\r\n\r\n\r\nIntroducing Machine Learning\r\nMachine learning algorithms can be classified by the method in which they are constructed. Supervised learning methods use training data to build a model, which is subsequently applied to additional data. On the other hand, unsupervised learning methods seek relationships among data points that can be leveraged to construct a model that is subsequently applied to the data of interest. In some cases, training data are used to validate the effectiveness of an unsupervised method, or perhaps to provide some level of supervision, which is known as semi-supervised learning.\r\nMore recently, additional types of learning have been developed. First, transfer learning extends a model trained on a previous data set to new, related data. This can be viewed as learning by analogy, which is similar to how humans learn. Second, reinforcement learning is a technique that explores how agents should behave within an environment by maximizing a cumulative reward. Finally, deep learning applies artificial neural networks that have multiple hidden layers to complex tasks, often with spectacular success in areas from image recognition to natural language processing.\r\nBroadly speaking, the application of a machine learning algorithm will be one of four different categories:\r\nClassification: generates a model that predicts discrete categories for new, unseen data.\r\nRegression: generates a model that predicts continuous values for new, unseen data.\r\nDimensionality reduction: identifies (and optionally ranks) the most important (potentially new) features (or dimensions) for a data set.\r\nClustering: identifies clusters of instances in an N-dimensional feature space.\r\n\r\nOne final point to clarify before proceeding with demonstrations of these different algorithm categories. When applying a machine learning algorithm to a problem, we often need to specify both model parameters and model hyperparameters. While they are similar, the difference between these two types of information depends on whether the value can be estimated from the data.\r\nParameter\r\nA value that can be estimated from the data being analyzed and that is internal to the machine learning algorithm. A parameter is generally not specified by the programmer, and instead is determined automatically by the algorithm implementation (e.g., directly in the caret package). For example, the coefficients in a linear regression model are machine learning parameters.\r\n\r\nHyperparameter\r\nA value that cannot be estimated from the data being analyzed and that is external to a specific machine learning algorithm. A hyperparameter is generally specified by the programmer prior to the start of the learning process. As a result, the hyperparameter directly influences the performance of the algorithm and thus is a tunable parameter. For example, the number of neighbors in a k-nearest neighbors implementation is a hyperparameter.\r\n\r\nIntroducing Caret3\r\nCaret stands for Classification And Regression Training. Apparently caret has little to do with our orange friend, the carrot. ü•ï\r\nNot only does caret allow you to run a plethora of ML methods, it also provides tools for auxiliary techniques such as:\r\nData preparation (imputation, centering/scaling data, removing correlated predictors, reducing skewness)\r\nData splitting\r\nVariable selection\r\nModel evaluation\r\nAn extensive vignette for caret can be found here: https://topepo.github.io/caret/index.html\r\nData Pre-Processing\r\nBefore we can apply a machine learning algorithm to the data of interest, we must divide the data into training and testing data sets. The training data are used to generate the supervised model, while the testing data are used to quantify the quality of the generated model. The function createDataPartition can be used to create balanced splits of the data. If the y argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. For example, to create a single 60/40% split of the iris data:\r\n\r\n\r\n#lets split the data 60/40\r\nlibrary(caret)\r\ntrainIndex <- createDataPartition(iris$Species, p = .6, list = FALSE, times = 1)\r\n\r\n#look at the first few\r\nhead(trainIndex)\r\n\r\n\r\n     Resample1\r\n[1,]         2\r\n[2,]         3\r\n[3,]         5\r\n[4,]         7\r\n[5,]         8\r\n[6,]        11\r\n\r\n#grab the data\r\nirisTrain <- iris[ trainIndex,]\r\nirisTest  <- iris[-trainIndex,]\r\n\r\n\r\n\r\nData Scaling\r\nMany machine learning  estimators are sensitive to variations in the spread of features within a data set. For example, if all features but one span similar ranges (e.g., zero through one) and one feature spans a much larger range (e.g., zero through one hundred), an algorithm might focus on the one feature with a larger spread, even if this produces a sub-optimal result. To prevent this, we generally scale the features to improve the performance of a given estimator.\r\nData scaling can take several forms:\r\nStandardization: the data are scaled to have zero mean and unit (i.e., one) variance.\r\nNormalization: the data are scaled to have unit mean and variance.\r\nRange: the data are scaled to span a defined range, such as [0,1].\r\nBinarization: the data are thresholded such that values below the threshold are zero (or False), and above the threshold are one (or True).\r\nOne important caveat to scaling is that any scaling technique should be trained via the fit method on the training data used for the machine learning algorithm. Once trained, the scaling technique can be applied equally to the training and testing data. In this manner, the testing data will always match the space spanned by the training data, which is what is used to generate the predictive model.\r\nWe demonstrate this approach in the following code cell, where we compute a standardization from our training data. This transformation is applied to both the training and testing data.\r\n\r\n\r\npreProcValues <- preProcess(irisTrain, method = c(\"center\", \"scale\"))\r\n\r\ntrainTransformed <- predict(preProcValues, irisTrain)\r\ntestTransformed <- predict(preProcValues, irisTest)\r\n\r\n\r\n\r\nI made a mistake here‚Ä¶Can you spot it üëÄ\r\n\r\n\r\npreProcValues <- preProcess(irisTest, method = c(\"center\", \"scale\"))\r\ntestTransformed <- predict(preProcValues, irisTest)\r\n\r\n\r\n\r\n\r\n\r\npsych::describe(trainTransformed)\r\n\r\n\r\n             vars  n mean   sd median trimmed  mad   min  max range\r\nSepal.Length    1 90    0 1.00  -0.03   -0.05 1.12 -1.91 2.35  4.27\r\nSepal.Width     2 90    0 1.00  -0.15    0.00 0.74 -2.64 2.33  4.97\r\nPetal.Length    3 90    0 1.00   0.33    0.01 0.93 -1.56 1.70  3.27\r\nPetal.Width     4 90    0 1.00   0.13   -0.02 1.36 -1.44 1.70  3.13\r\nSpecies*        5 90    2 0.82   2.00    2.00 1.48  1.00 3.00  2.00\r\n              skew kurtosis   se\r\nSepal.Length  0.34    -0.55 0.11\r\nSepal.Width  -0.02    -0.09 0.11\r\nPetal.Length -0.29    -1.43 0.11\r\nPetal.Width  -0.06    -1.39 0.11\r\nSpecies*      0.00    -1.53 0.09\r\n\r\npsych::describe(testTransformed)\r\n\r\n\r\n             vars  n mean   sd median trimmed  mad   min  max range\r\nSepal.Length    1 60    0 1.00  -0.08   -0.03 1.18 -1.67 2.31  3.98\r\nSepal.Width     2 60    0 1.00  -0.10   -0.07 0.92 -1.75 2.78  4.53\r\nPetal.Length    3 60    0 1.00   0.33   -0.01 0.98 -1.38 1.71  3.10\r\nPetal.Width     4 60    0 1.00   0.20   -0.01 1.26 -1.44 1.70  3.14\r\nSpecies*        5 60    2 0.82   2.00    2.00 1.48  1.00 3.00  2.00\r\n              skew kurtosis   se\r\nSepal.Length  0.25    -0.78 0.13\r\nSepal.Width   0.60     0.08 0.13\r\nPetal.Length -0.25    -1.46 0.13\r\nPetal.Width  -0.16    -1.37 0.13\r\nSpecies*      0.00    -1.55 0.11\r\n\r\nWith our data properly divided into training and testing samples, and the features appropriately scaled, we now change to the application of machine learning algorithms\r\nClassification\r\nThe first type of algorithm we will demonstrate is classification, where we train an estimator to generate a model for the prediction of discrete labels. The following code cell completes this task by performing k-Nearest Neighbors classification. In this example, we use five nearest neighbors (but this value can be easily adjusted to see how the classification performance changes). As demonstrated in this code example, the standard classification process in caret is to first fit a model to the training data and to subsequently apply this model to predict values for the testing data. We can compute an accuracy measurement for our trained algorithm to compare the predicted and known labels for the testing data.\r\nSince we set the k there is no reason to actually train‚Ä¶ üòñ\r\n\r\n\r\n#fit knn\r\nknn_fit<-train(Species~.,\r\n               data=trainTransformed,\r\n               method=\"knn\",\r\n               tuneGrid=data.frame(k=5))\r\n\r\nknn_fit\r\n\r\n\r\nk-Nearest Neighbors \r\n\r\n90 samples\r\n 4 predictor\r\n 3 classes: 'setosa', 'versicolor', 'virginica' \r\n\r\nNo pre-processing\r\nResampling: Bootstrapped (25 reps) \r\nSummary of sample sizes: 90, 90, 90, 90, 90, 90, ... \r\nResampling results:\r\n\r\n  Accuracy   Kappa    \r\n  0.9416329  0.9111908\r\n\r\nTuning parameter 'k' was held constant at a value of 5\r\n\r\n\r\n\r\n#predict on the test set\r\nknn_pred<-predict(knn_fit,testTransformed)\r\n\r\n#confusion\r\nconfusionMatrix(knn_pred,testTransformed$Species)\r\n\r\n\r\nConfusion Matrix and Statistics\r\n\r\n            Reference\r\nPrediction   setosa versicolor virginica\r\n  setosa         20          0         0\r\n  versicolor      0         17         2\r\n  virginica       0          3        18\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9167          \r\n                 95% CI : (0.8161, 0.9724)\r\n    No Information Rate : 0.3333          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.875           \r\n                                          \r\n Mcnemar's Test P-Value : NA              \r\n\r\nStatistics by Class:\r\n\r\n                     Class: setosa Class: versicolor Class: virginica\r\nSensitivity                 1.0000            0.8500           0.9000\r\nSpecificity                 1.0000            0.9500           0.9250\r\nPos Pred Value              1.0000            0.8947           0.8571\r\nNeg Pred Value              1.0000            0.9268           0.9487\r\nPrevalence                  0.3333            0.3333           0.3333\r\nDetection Rate              0.3333            0.2833           0.3000\r\nDetection Prevalence        0.3333            0.3167           0.3500\r\nBalanced Accuracy           1.0000            0.9000           0.9125\r\n\r\nRegression\r\nThe second machine learning application we will demonstrate is regression. To demonstrate regression, we will introduce the Decision Tree. A decision tree simply asks a set of questions of the data, and based on the answers, constructs a model representation. The tree (or model) is constructed by recursively splitting a data set into new groupings based on a statistical measure of the data along each different dimension (popular measures include the Gini coefficient or the entropy).\r\nThe terminal nodes in the tree are known as leaf nodes, and they provide the final predictions. In the simplest form, the leaf node simply provides the final prediction. More realistic decision trees generate a model prediction by using all instances in the leaf node, for example by averaging across them.\r\nBefore generating a regression model, however, we must pre-process our data to identify our independent variables (or features) and our dependent variable (or feature). Given a set of new independent variables, a regression model will predict the dependent variable. In the following code cell, we first select the first three features to be our independent variables and the fourth variable to be our dependent variable. We divide these into training and testing samples.\r\n\r\n\r\n#fit Decision Tree\r\nDT_fit1<-train(Petal.Width~Sepal.Length+Sepal.Width+Petal.Length,\r\n               data=trainTransformed,\r\n               method=\"rpart\")\r\n\r\nDT_fit1\r\n\r\n\r\nCART \r\n\r\n90 samples\r\n 3 predictor\r\n\r\nNo pre-processing\r\nResampling: Bootstrapped (25 reps) \r\nSummary of sample sizes: 90, 90, 90, 90, 90, 90, ... \r\nResampling results across tuning parameters:\r\n\r\n  cp          RMSE       Rsquared   MAE      \r\n  0.03205658  0.3274900  0.8964702  0.2547857\r\n  0.11298627  0.3969570  0.8468579  0.3221264\r\n  0.76950143  0.8393752  0.7564941  0.7129634\r\n\r\nRMSE was used to select the optimal model using the smallest value.\r\nThe final value used for the model was cp = 0.03205658.\r\n\r\nInstall another package\r\n\r\n\r\ninstall.packages(\"rpart.plot\")\r\n\r\n\r\n\r\nWe can plot simple trees\r\n\r\n\r\nrpart.plot::prp(DT_fit1$finalModel,box.palette = \"Reds\", tweak = 1.2)\r\n\r\n\r\n\r\n\r\nLets predict\r\n\r\n\r\n#predict on the test set\r\nDTfit1_pred<-predict(DT_fit1,testTransformed)\r\n\r\n\r\n\r\nRsquared üòà\r\n\r\n\r\npreds<-DTfit1_pred\r\nactual<-testTransformed$Petal.Width\r\n#one formulation\r\nrss <- sum((preds - actual) ^ 2)  ## residual sum of squares\r\ntss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares\r\nrsq <- 1 - rss/tss\r\nrsq\r\n\r\n\r\n[1] 0.8609195\r\n\r\n#another\r\nregss <- sum((preds - mean(preds)) ^ 2) ## regression sum of squares\r\nregss / tss\r\n\r\n\r\n[1] 0.8808349\r\n\r\n#another\r\ncor(preds,actual)^2\r\n\r\n\r\n[1] 0.8610817\r\n\r\nTime to blow RSquared up4 üí•\r\n\r\nR-squared is a statistic that often accompanies regression output. It ranges in value from 0 to 1 and is usually interpreted as summarizing the percent of variation in the response that the regression model explains. So an R-squared of 0.65 might mean that the model explains about 65% of the variation in our dependent variable. Given this logic, we prefer our regression models have a high R-squared.\r\nIn R, we typically get R-squared by calling the summary function on a model object. Here‚Äôs a quick example using simulated data:\r\n\r\n\r\n# independent variable\r\nx <- 1:20 \r\n# for reproducibility\r\nset.seed(1) \r\n# dependent variable; function of x with random error\r\ny <- 2 + 0.5*x + rnorm(20,0,3) \r\n# simple linear regression\r\nmod <- lm(y~x)\r\n# request just the r-squared value\r\nsummary(mod)$r.squared          \r\n\r\n\r\n[1] 0.6026682\r\n\r\nOne way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations:\r\n\\[\r\nR^{2} =  \\frac{\\sum (\\hat{y} ‚Äì \\bar{\\hat{y}})^{2}}{\\sum (y ‚Äì \\bar{y})^{2}}\r\n\\]\r\nWe can calculate it directly using our model object like so:\r\n\r\n\r\n# extract fitted (or predicted) values from model\r\nf <- mod$fitted.values\r\n# sum of squared fitted-value deviations\r\nmss <- sum((f - mean(f))^2)\r\n# sum of squared original-value deviations\r\ntss <- sum((y - mean(y))^2)\r\n# r-squared\r\nmss/tss                      \r\n\r\n\r\n[1] 0.6026682\r\n\r\n1. R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making\\(œÉ^2\\) large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.\r\nWhat is \\(œÉ^2\\)? When we perform linear regression, we assume our model almost predicts our dependent variable. The difference between ‚Äúalmost‚Äù and ‚Äúexact‚Äù is assumed to be a draw from a Normal distribution with mean 0 and some variance we call \\(œÉ^2\\).\r\nThis statement is easy enough to demonstrate. The way we do it here is to create a function that (1) generates data meeting the assumptions of simple linear regression (independent observations, normally distributed errors with constant variance), (2) fits a simple linear model to the data, and (3) reports the R-squared. Notice the only parameter for sake of simplicity is sigma. We then ‚Äúapply‚Äù this function to a series of increasing \\(œÉ\\) values and plot the results.\r\n\r\n\r\nr2.0 <- function(sig){\r\n  # our predictor\r\n  x <- seq(1,10,length.out = 100)   \r\n  # our response; a function of x plus some random noise\r\n  y <- 2 + 1.2*x + rnorm(100,0,sd = sig) \r\n  # print the R-squared value\r\n  summary(lm(y ~ x))$r.squared          \r\n}\r\n\r\nsigmas <- seq(0.5,20,length.out = 20)\r\n # apply our function to a series of sigma values\r\nrout <- sapply(sigmas, r2.0)            \r\nplot(rout ~ sigmas, type=\"b\")\r\n\r\n\r\n\r\n\r\nR-squared tanks hard with increasing sigma, even though the model is completely correct in every respect.\r\nR-squared can be arbitrarily close to 1 when the model is totally wrong.\r\nThe point being made is that R-squared does not measure goodness of fit.\r\n\r\n\r\nset.seed(1)\r\n# our predictor is data from an exponential distribution\r\nx <- rexp(50,rate=0.005)\r\n# non-linear data generation\r\ny <- (x-1)^2 * runif(50, min=0.8, max=1.2) \r\n# clearly non-linear\r\nplot(x,y)             \r\n\r\n\r\n\r\n\r\n\r\n\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.8485146\r\n\r\nIt‚Äôs very high at about 0.85, but the model is completely wrong. Using R-squared to justify the ‚Äúgoodness‚Äù of our model in this instance would be a mistake. Hopefully one would plot the data first and recognize that a simple linear regression in this case would be inappropriate.\r\n3. R-squared says nothing about prediction error, even with \\(œÉ^2\\) exactly the same, and no change in the coefficients. R-squared can be anywhere between 0 and 1 just by changing the range of X. We‚Äôre better off using Mean Square Error (MSE) as a measure of prediction error.\r\nMSE is basically the fitted y values minus the observed y values, squared, then summed, and then divided by the number of observations.\r\nLet‚Äôs demonstrate this statement by first generating data that meets all simple linear regression assumptions and then regressing y on x to assess both R-squared and MSE.\r\n\r\n\r\nx <- seq(1,10,length.out = 100)\r\nset.seed(1)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\r\nmod1 <- lm(y ~ x)\r\nsummary(mod1)$r.squared\r\n\r\n\r\n[1] 0.9383379\r\n\r\n# Mean squared error\r\nsum((fitted(mod1) - y)^2)/100\r\n\r\n\r\n[1] 0.6468052\r\n\r\nNow repeat the above code, but this time with a different range of x. Leave everything else the same:\r\n\r\n\r\n # new range of x\r\nx <- seq(1,2,length.out = 100)      \r\nset.seed(1)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\r\nmod1 <- lm(y ~ x)\r\nsummary(mod1)$r.squared\r\n\r\n\r\n[1] 0.1502448\r\n\r\n# Mean squared error\r\nsum((fitted(mod1) - y)^2)/100        \r\n\r\n\r\n[1] 0.6468052\r\n\r\nThe R-squared falls from 0.94 to 0.15 but the MSE remains the same. In other words the predictive ability is the same for both data sets, but the R-squared would lead you to believe the first example somehow had a model with more predictive power.\r\nR-squared can easily go down when the model assumptions are better fulfilled.\r\nLet‚Äôs examine this by generating data that would benefit from transformation. Notice the R code below is very much like our previous efforts but now we exponentiate our y variable.\r\n\r\n\r\nx <- seq(1,2,length.out = 100)\r\nset.seed(1)\r\ny <- exp(-2 - 0.09*x + rnorm(100,0,sd = 2.5))\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.003281718\r\n\r\nplot(lm(y ~ x), which=3)\r\n\r\n\r\n\r\n\r\nR-squared is very low and our residuals vs.¬†fitted plot reveals outliers and non-constant variance. A common fix for this is to log transform the data. Let‚Äôs try that and see what happens:\r\n\r\n\r\nplot(lm(log(y)~x),which = 3) \r\n\r\n\r\n\r\n\r\nThe diagnostic plot looks much better. Our assumption of constant variance appears to be met. But look at the R-squared:\r\n\r\n\r\nsummary(lm(log(y)~x))$r.squared \r\n\r\n\r\n[1] 0.0006921086\r\n\r\nIt‚Äôs even lower! This is an extreme case and it doesn‚Äôt always happen like this. In fact, a log transformation will usually produce an increase in R-squared. But as just demonstrated, assumptions that are better fulfilled don‚Äôt always lead to higher R-squared.\r\nIt is very common to say that R-squared is ‚Äúthe fraction of variance explained‚Äù by the regression. [Yet] if we regressed X on Y, we‚Äôd get exactly the same R-squared. This in itself should be enough to show that a high R-squared says nothing about explaining one variable by another.\r\nThis is the easiest statement to demonstrate:\r\n\r\n\r\nx <- seq(1,10,length.out = 100)\r\ny <- 2 + 1.2*x + rnorm(100,0,sd = 2)\r\nsummary(lm(y ~ x))$r.squared\r\n\r\n\r\n[1] 0.737738\r\n\r\nsummary(lm(x ~ y))$r.squared\r\n\r\n\r\n[1] 0.737738\r\n\r\nDoes x explain y, or does y explain x? Are we saying ‚Äúexplain‚Äù to dance around the word ‚Äúcause‚Äù? In a simple scenario with two variables such as this, R-squared is simply the square of the correlation between x and y:\r\n\r\n\r\nall.equal(cor(x,y)^2, summary(lm(x ~ y))$r.squared, summary(lm(y ~ x))$r.squared)\r\n\r\n\r\n[1] TRUE\r\n\r\nLet‚Äôs recap:\r\nR-squared does not measure goodness of fit.\r\nR-squared does not measure predictive error.\r\nR-squared does not necessarily increase when assumptions are better satisfied.\r\nR-squared does not measure how one variable explains another.\r\nDimensionality Reduction\r\nWhen confronted with a large, multi-dimensional data set, one approach to simplify any subsequent analysis is to reduce the number of dimensions (or features) that must be processed. In some cases, features can be removed from an analysis based on business logic, or the features that contain the most information can be quantified somehow. More generally, however, we can employ dimensional reduction, a machine learning technique that quantifies relationships between the original dimensions (or features, attributes, or columns of a DataFrame) to identify new dimensions that better capture the inherent relationships within the data.\r\nThe standard technique to perform this is known as principal component analysis, or PCA. Mathematically, we can derive PCA by using linear algebra to solve a set of linear equations. This process effectively rotates the data into a new set of dimensions, and by ranking the importance of the new dimensions, we can optimally select fewer dimensions for use in other machine learning algorithms.\r\nThe PCA estimator requires one tunable hyper-parameter that specifies the target number of dimensions. This value can be arbitrarily selected, perhaps based on prior information, or it can be iteratively determined. After the model is created, we fit the model to the data and next create our new, rotated data set. This is demonstrated in the next code cell.\r\n\r\n\r\nlibrary(caret)\r\n#store our data in another object\r\ndat <- iris\r\n#take the 4 continuous variables and perform PCA\r\ncaret.pca <- preProcess(dat[,-5], method=\"pca\",pcaComp=2)\r\n\r\ncaret.pca\r\n\r\n\r\nCreated from 150 samples and 4 variables\r\n\r\nPre-processing:\r\n  - centered (4)\r\n  - ignored (0)\r\n  - principal component signal extraction (4)\r\n  - scaled (4)\r\n\r\nPCA used 2 components as specified\r\n\r\ncaret.pca$\r\n#use that data to form our new inputs\r\ndat2 <- predict(caret.pca, dat[,-5])\r\n\r\n\r\n#using stats\r\nstat.pca <- prcomp(dat[,-5],\r\n                 center = TRUE,\r\n                 scale. = TRUE) \r\n\r\n# plot method\r\nplot(stat.pca, type = \"l\")\r\n\r\n\r\n\r\nsummary(stat.pca)\r\n\r\n\r\nImportance of components:\r\n                          PC1    PC2     PC3     PC4\r\nStandard deviation     1.7084 0.9560 0.38309 0.14393\r\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\r\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\r\n\r\nBelow is a graphical representation5\r\n\r\n\r\n\r\n\r\n\r\n\r\nAt the end of the previous code cell, we measure the amount of the original variance (or spread) in the original data that is captured by each new dimension. As this example shows, these two new dimensions capture almost 96% of the variance in the original data. This means that any analysis that uses only these two new dimensions will closely represent the analysis if performed on the entire data.\r\n\r\nClustering\r\nThe last machine learning technique we will explore in this notebook is cluster finding. In this introductory notebook, we will demonstrate one of the simplest clustering techniques, spatial clustering, which seeks to first find NN clusters in a data set and to subsequently identify to which cluster each instance (or data point) belongs. The specific algorithm we employ below is the k-means algorithm, which is one of the simplest to understand. In this algorithm, we start with a guess for the number of clusters (again this can be based on prior information or iteratively quantified). We randomly place cluster centers in the data and determine how well the data cluster to these cluster centers. This information is used to pick new cluster centers, and the process continues until a solution converges (or we reach a predefined number of iterations).\r\n\r\n\r\nClusters<-kmeans(trainTransformed[,-5],centers=3)\r\n\r\nClusters\r\n\r\n\r\nK-means clustering with 3 clusters of sizes 33, 27, 30\r\n\r\nCluster means:\r\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\r\n1  -0.02065506  -0.9227411    0.3702566   0.2968749\r\n2   1.14810864   0.1399005    0.9929469   1.0062961\r\n3  -1.01057722   0.8891048   -1.3009345  -1.2322289\r\n\r\nClustering vector:\r\n  2   3   5   7   8  11  13  14  15  17  18  20  21  23  24  25  26 \r\n  3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \r\n 27  28  31  32  36  37  43  44  45  46  47  49  50  52  55  56  57 \r\n  3   3   3   3   3   3   3   3   3   3   3   3   3   2   1   1   2 \r\n 58  61  63  68  70  71  72  75  76  77  78  79  80  82  83  85  86 \r\n  1   1   1   1   1   2   1   1   2   2   2   1   1   1   1   1   2 \r\n 87  89  91  92  94  95  96  98 100 101 103 104 105 111 114 115 116 \r\n  2   1   1   1   1   1   1   1   1   2   2   2   2   2   1   1   2 \r\n118 120 121 122 123 124 126 127 130 131 133 134 136 139 140 143 144 \r\n  2   1   2   1   2   1   2   1   2   2   2   1   2   1   2   1   2 \r\n145 146 147 149 150 \r\n  2   2   1   2   1 \r\n\r\nWithin cluster sum of squares by cluster:\r\n[1] 31.84104 27.02680 20.82988\r\n (between_SS / total_SS =  77.6 %)\r\n\r\nAvailable components:\r\n\r\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"    \r\n[5] \"tot.withinss\" \"betweenss\"    \"size\"         \"iter\"        \r\n[9] \"ifault\"      \r\n\r\nThe above list is an output of the kmeans() function. Let‚Äôs see some of the important ones closely:\r\ncluster: a vector of integers (from 1:k) indicating the cluster to which each point is allocated.\r\ncenters: a matrix of cluster centers.\r\nwithinss: vector of within-cluster sum of squares, one component per cluster.\r\ntot.withinss: total within-cluster sum of squares. That is, sum(withinss).\r\nsize: the number of points in each cluster.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\nClusterdata<-trainTransformed\r\nClusterdata$Cluster<-as.factor(Clusters$cluster)\r\n\r\n#view the whole dataset\r\nknitr::kable(Clusterdata)%>%\r\n  kableExtra::kable_styling(\"striped\")%>%\r\n  kableExtra::scroll_box(width = \"100%\",height=\"300px\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nSepal.Length\r\n\r\n\r\nSepal.Width\r\n\r\n\r\nPetal.Length\r\n\r\n\r\nPetal.Width\r\n\r\n\r\nSpecies\r\n\r\n\r\nCluster\r\n\r\n\r\n2\r\n\r\n\r\n-1.1611184\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n-1.3334102\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n-1.4120203\r\n\r\n\r\n0.3423882\r\n\r\n\r\n-1.3907200\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n5\r\n\r\n\r\n-1.0356674\r\n\r\n\r\n1.3364183\r\n\r\n\r\n-1.3334102\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n7\r\n\r\n\r\n-1.5374713\r\n\r\n\r\n0.8394033\r\n\r\n\r\n-1.3334102\r\n\r\n\r\n-1.1756248\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n8\r\n\r\n\r\n-1.0356674\r\n\r\n\r\n0.8394033\r\n\r\n\r\n-1.2761003\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n11\r\n\r\n\r\n-0.5338636\r\n\r\n\r\n1.5849259\r\n\r\n\r\n-1.2761003\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n13\r\n\r\n\r\n-1.2865693\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n-1.3334102\r\n\r\n\r\n-1.4368747\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n14\r\n\r\n\r\n-1.9138242\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n-1.5053398\r\n\r\n\r\n-1.4368747\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n15\r\n\r\n\r\n-0.0320597\r\n\r\n\r\n2.3304485\r\n\r\n\r\n-1.4480299\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n17\r\n\r\n\r\n-0.5338636\r\n\r\n\r\n2.0819410\r\n\r\n\r\n-1.3907200\r\n\r\n\r\n-1.0449998\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n18\r\n\r\n\r\n-0.9102164\r\n\r\n\r\n1.0879108\r\n\r\n\r\n-1.3334102\r\n\r\n\r\n-1.1756248\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n20\r\n\r\n\r\n-0.9102164\r\n\r\n\r\n1.8334334\r\n\r\n\r\n-1.2761003\r\n\r\n\r\n-1.1756248\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n21\r\n\r\n\r\n-0.5338636\r\n\r\n\r\n0.8394033\r\n\r\n\r\n-1.1614805\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n23\r\n\r\n\r\n-1.5374713\r\n\r\n\r\n1.3364183\r\n\r\n\r\n-1.5626497\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n24\r\n\r\n\r\n-0.9102164\r\n\r\n\r\n0.5908957\r\n\r\n\r\n-1.1614805\r\n\r\n\r\n-0.9143748\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n25\r\n\r\n\r\n-1.2865693\r\n\r\n\r\n0.8394033\r\n\r\n\r\n-1.0468607\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n26\r\n\r\n\r\n-1.0356674\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n-1.2187904\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n27\r\n\r\n\r\n-1.0356674\r\n\r\n\r\n0.8394033\r\n\r\n\r\n-1.2187904\r\n\r\n\r\n-1.0449998\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n28\r\n\r\n\r\n-0.7847655\r\n\r\n\r\n1.0879108\r\n\r\n\r\n-1.2761003\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n31\r\n\r\n\r\n-1.2865693\r\n\r\n\r\n0.0938806\r\n\r\n\r\n-1.2187904\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n32\r\n\r\n\r\n-0.5338636\r\n\r\n\r\n0.8394033\r\n\r\n\r\n-1.2761003\r\n\r\n\r\n-1.0449998\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n36\r\n\r\n\r\n-1.0356674\r\n\r\n\r\n0.3423882\r\n\r\n\r\n-1.4480299\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n37\r\n\r\n\r\n-0.4084126\r\n\r\n\r\n1.0879108\r\n\r\n\r\n-1.3907200\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n43\r\n\r\n\r\n-1.7883732\r\n\r\n\r\n0.3423882\r\n\r\n\r\n-1.3907200\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n44\r\n\r\n\r\n-1.0356674\r\n\r\n\r\n1.0879108\r\n\r\n\r\n-1.2187904\r\n\r\n\r\n-0.7837498\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n45\r\n\r\n\r\n-0.9102164\r\n\r\n\r\n1.8334334\r\n\r\n\r\n-1.0468607\r\n\r\n\r\n-1.0449998\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n46\r\n\r\n\r\n-1.2865693\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n-1.3334102\r\n\r\n\r\n-1.1756248\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n47\r\n\r\n\r\n-0.9102164\r\n\r\n\r\n1.8334334\r\n\r\n\r\n-1.2187904\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n49\r\n\r\n\r\n-0.6593145\r\n\r\n\r\n1.5849259\r\n\r\n\r\n-1.2761003\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n50\r\n\r\n\r\n-1.0356674\r\n\r\n\r\n0.5908957\r\n\r\n\r\n-1.3334102\r\n\r\n\r\n-1.3062497\r\n\r\n\r\nsetosa\r\n\r\n\r\n3\r\n\r\n\r\n52\r\n\r\n\r\n0.7206461\r\n\r\n\r\n0.3423882\r\n\r\n\r\n0.4431965\r\n\r\n\r\n0.3918749\r\n\r\n\r\nversicolor\r\n\r\n\r\n2\r\n\r\n\r\n55\r\n\r\n\r\n0.8460971\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n0.5005064\r\n\r\n\r\n0.3918749\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n56\r\n\r\n\r\n-0.1575107\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n0.4431965\r\n\r\n\r\n0.1306250\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n57\r\n\r\n\r\n0.5951951\r\n\r\n\r\n0.5908957\r\n\r\n\r\n0.5578163\r\n\r\n\r\n0.5224999\r\n\r\n\r\nversicolor\r\n\r\n\r\n2\r\n\r\n\r\n58\r\n\r\n\r\n-1.1611184\r\n\r\n\r\n-1.6456722\r\n\r\n\r\n-0.2445222\r\n\r\n\r\n-0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n61\r\n\r\n\r\n-1.0356674\r\n\r\n\r\n-2.6397023\r\n\r\n\r\n-0.1299024\r\n\r\n\r\n-0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n63\r\n\r\n\r\n0.2188422\r\n\r\n\r\n-2.1426872\r\n\r\n\r\n0.1566470\r\n\r\n\r\n-0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n68\r\n\r\n\r\n-0.0320597\r\n\r\n\r\n-0.9001495\r\n\r\n\r\n0.2139569\r\n\r\n\r\n-0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n70\r\n\r\n\r\n-0.2829616\r\n\r\n\r\n-1.3971646\r\n\r\n\r\n0.0993371\r\n\r\n\r\n-0.1306250\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n71\r\n\r\n\r\n0.0933913\r\n\r\n\r\n0.3423882\r\n\r\n\r\n0.6151262\r\n\r\n\r\n0.7837498\r\n\r\n\r\nversicolor\r\n\r\n\r\n2\r\n\r\n\r\n72\r\n\r\n\r\n0.3442932\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n0.1566470\r\n\r\n\r\n0.1306250\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n75\r\n\r\n\r\n0.7206461\r\n\r\n\r\n-0.4031345\r\n\r\n\r\n0.3285767\r\n\r\n\r\n0.1306250\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n76\r\n\r\n\r\n0.9715480\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n0.3858866\r\n\r\n\r\n0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n2\r\n\r\n\r\n77\r\n\r\n\r\n1.2224500\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n0.6151262\r\n\r\n\r\n0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n2\r\n\r\n\r\n78\r\n\r\n\r\n1.0969990\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n0.7297460\r\n\r\n\r\n0.6531249\r\n\r\n\r\nversicolor\r\n\r\n\r\n2\r\n\r\n\r\n79\r\n\r\n\r\n0.2188422\r\n\r\n\r\n-0.4031345\r\n\r\n\r\n0.4431965\r\n\r\n\r\n0.3918749\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n80\r\n\r\n\r\n-0.1575107\r\n\r\n\r\n-1.1486571\r\n\r\n\r\n-0.1299024\r\n\r\n\r\n-0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n82\r\n\r\n\r\n-0.4084126\r\n\r\n\r\n-1.6456722\r\n\r\n\r\n-0.0152826\r\n\r\n\r\n-0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n83\r\n\r\n\r\n-0.0320597\r\n\r\n\r\n-0.9001495\r\n\r\n\r\n0.0993371\r\n\r\n\r\n0.0000000\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n85\r\n\r\n\r\n-0.5338636\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n0.4431965\r\n\r\n\r\n0.3918749\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n86\r\n\r\n\r\n0.2188422\r\n\r\n\r\n0.8394033\r\n\r\n\r\n0.4431965\r\n\r\n\r\n0.5224999\r\n\r\n\r\nversicolor\r\n\r\n\r\n2\r\n\r\n\r\n87\r\n\r\n\r\n1.0969990\r\n\r\n\r\n0.0938806\r\n\r\n\r\n0.5578163\r\n\r\n\r\n0.3918749\r\n\r\n\r\nversicolor\r\n\r\n\r\n2\r\n\r\n\r\n89\r\n\r\n\r\n-0.2829616\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n0.2139569\r\n\r\n\r\n0.1306250\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n91\r\n\r\n\r\n-0.4084126\r\n\r\n\r\n-1.1486571\r\n\r\n\r\n0.3858866\r\n\r\n\r\n0.0000000\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n92\r\n\r\n\r\n0.3442932\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n0.5005064\r\n\r\n\r\n0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n94\r\n\r\n\r\n-1.0356674\r\n\r\n\r\n-1.8941797\r\n\r\n\r\n-0.2445222\r\n\r\n\r\n-0.2612499\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n95\r\n\r\n\r\n-0.2829616\r\n\r\n\r\n-0.9001495\r\n\r\n\r\n0.2712668\r\n\r\n\r\n0.1306250\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n96\r\n\r\n\r\n-0.1575107\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n0.2712668\r\n\r\n\r\n0.0000000\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n98\r\n\r\n\r\n0.4697442\r\n\r\n\r\n-0.4031345\r\n\r\n\r\n0.3285767\r\n\r\n\r\n0.1306250\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n100\r\n\r\n\r\n-0.1575107\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n0.2139569\r\n\r\n\r\n0.1306250\r\n\r\n\r\nversicolor\r\n\r\n\r\n1\r\n\r\n\r\n101\r\n\r\n\r\n0.5951951\r\n\r\n\r\n0.5908957\r\n\r\n\r\n1.3028449\r\n\r\n\r\n1.6981246\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n103\r\n\r\n\r\n1.5988029\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n1.2455350\r\n\r\n\r\n1.1756248\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n104\r\n\r\n\r\n0.5951951\r\n\r\n\r\n-0.4031345\r\n\r\n\r\n1.0736053\r\n\r\n\r\n0.7837498\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n105\r\n\r\n\r\n0.8460971\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n1.1882251\r\n\r\n\r\n1.3062497\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n111\r\n\r\n\r\n0.8460971\r\n\r\n\r\n0.3423882\r\n\r\n\r\n0.7870558\r\n\r\n\r\n1.0449998\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n114\r\n\r\n\r\n-0.1575107\r\n\r\n\r\n-1.3971646\r\n\r\n\r\n0.7297460\r\n\r\n\r\n1.0449998\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n115\r\n\r\n\r\n-0.0320597\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n0.7870558\r\n\r\n\r\n1.5674997\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n116\r\n\r\n\r\n0.7206461\r\n\r\n\r\n0.3423882\r\n\r\n\r\n0.9016756\r\n\r\n\r\n1.4368747\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n118\r\n\r\n\r\n2.3515086\r\n\r\n\r\n1.8334334\r\n\r\n\r\n1.7040141\r\n\r\n\r\n1.3062497\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n120\r\n\r\n\r\n0.2188422\r\n\r\n\r\n-2.1426872\r\n\r\n\r\n0.7297460\r\n\r\n\r\n0.3918749\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n121\r\n\r\n\r\n1.3479009\r\n\r\n\r\n0.3423882\r\n\r\n\r\n1.1309152\r\n\r\n\r\n1.4368747\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n122\r\n\r\n\r\n-0.2829616\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n0.6724361\r\n\r\n\r\n1.0449998\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n123\r\n\r\n\r\n2.3515086\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n1.7040141\r\n\r\n\r\n1.0449998\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n124\r\n\r\n\r\n0.5951951\r\n\r\n\r\n-0.9001495\r\n\r\n\r\n0.6724361\r\n\r\n\r\n0.7837498\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n126\r\n\r\n\r\n1.7242538\r\n\r\n\r\n0.3423882\r\n\r\n\r\n1.3028449\r\n\r\n\r\n0.7837498\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n127\r\n\r\n\r\n0.4697442\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n0.6151262\r\n\r\n\r\n0.7837498\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n130\r\n\r\n\r\n1.7242538\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n1.1882251\r\n\r\n\r\n0.5224999\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n131\r\n\r\n\r\n1.9751557\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n1.3601548\r\n\r\n\r\n0.9143748\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n133\r\n\r\n\r\n0.7206461\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n1.0736053\r\n\r\n\r\n1.3062497\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n134\r\n\r\n\r\n0.5951951\r\n\r\n\r\n-0.6516420\r\n\r\n\r\n0.7870558\r\n\r\n\r\n0.3918749\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n136\r\n\r\n\r\n2.3515086\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n1.3601548\r\n\r\n\r\n1.4368747\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n139\r\n\r\n\r\n0.2188422\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n0.6151262\r\n\r\n\r\n0.7837498\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n140\r\n\r\n\r\n1.3479009\r\n\r\n\r\n0.0938806\r\n\r\n\r\n0.9589855\r\n\r\n\r\n1.1756248\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n143\r\n\r\n\r\n-0.0320597\r\n\r\n\r\n-0.9001495\r\n\r\n\r\n0.7870558\r\n\r\n\r\n0.9143748\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n144\r\n\r\n\r\n1.2224500\r\n\r\n\r\n0.3423882\r\n\r\n\r\n1.2455350\r\n\r\n\r\n1.4368747\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n145\r\n\r\n\r\n1.0969990\r\n\r\n\r\n0.5908957\r\n\r\n\r\n1.1309152\r\n\r\n\r\n1.6981246\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n146\r\n\r\n\r\n1.0969990\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n0.8443657\r\n\r\n\r\n1.4368747\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n147\r\n\r\n\r\n0.5951951\r\n\r\n\r\n-1.3971646\r\n\r\n\r\n0.7297460\r\n\r\n\r\n0.9143748\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n149\r\n\r\n\r\n0.4697442\r\n\r\n\r\n0.8394033\r\n\r\n\r\n0.9589855\r\n\r\n\r\n1.4368747\r\n\r\n\r\nvirginica\r\n\r\n\r\n2\r\n\r\n\r\n150\r\n\r\n\r\n0.0933913\r\n\r\n\r\n-0.1546269\r\n\r\n\r\n0.7870558\r\n\r\n\r\n0.7837498\r\n\r\n\r\nvirginica\r\n\r\n\r\n1\r\n\r\n\r\n\r\n\r\n\r\n#Remember me\r\nggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Cluster))+geom_point(alpha=0.5)\r\n\r\n\r\n\r\nggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Cluster))+geom_point(alpha=0.5)+facet_wrap(~Species)\r\n\r\n\r\n\r\nggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Species))+geom_point(alpha=0.5) + \r\n   geom_point(data=as.data.frame(Clusters$centers), aes(color=\"Cluster center\"), size=5) + theme(legend.title = element_blank())+ggtitle(\"Iris Cluster Demonstration\")\r\n\r\n\r\n\r\n\r\nExercise 1\r\nUsing the code above, answer the following questions.\r\nChange the p=.6 to p=.75 in the [Data Pre-Processing] section. How did the classification results change?\r\nChange the p=.6 to p=.4 in the [Data Pre-Processing] section. How did the classification results change?\r\nChange the k hyper-parameter in the k-nn estimator to three (and ten). In other words change the 5 to 10 in tuneGrid=data.frame(k=5) in the [Classification] section. How did the classification results change?\r\nChange the p=.6 to p=.75 in the [Data Pre-Processing] section. How did the regression results change?\r\nChange the pcaComp hyper-parameter in the PCA code example to three (and four) in the [Dimensionality Reduction] section. What are the new explained variances?\r\nChange the centers hyper-parameter in the cluster finding code example to two (and four) in the [Clustering] section. Where are the new cluster centers? Does this look better or worse?\r\nWhat does the set.seed function in R do? Why use it? Should we have used it above?\r\nModel Persistence\r\nAs the previous code cells demonstrate, we can generate machine learning models rather easily for small data sets by using the caret library. For larger data sets, however, either in the number of instances, the number of features, or both, building a quality machine learning model can take considerable time and effort. As a result, we may wish to persist a trained machine learning model so that it can be applied to new data at a later time.\r\n\r\n\r\n#save our model\r\nsave(DT_fit1,file = \"DT_fit1.Rda\")\r\n\r\n#remove it from  the environment\r\nrm(DT_fit1)\r\n\r\n\r\n\r\n\r\n\r\n#load our model\r\nload(file = \"DT_fit1.Rda\")\r\n\r\n\r\n\r\nWhenever you load it back you can use it just like before.\r\nDeep Dive\r\n\r\n\r\nRegression\r\nRegression is awesome\r\n\r\n\r\nK-NN\r\nK-NN is simple, but awesome\r\n\r\n\r\nRegression\r\nIntroduction to Linear Regression\r\nThis lesson builds on the ordinary linear regression concept, introduced in business statistics, to discuss linear regression as a machine learning task. Regression, or the creation of a predictive model from data, is one of the key machine learning tasks. By using linear regression, which can often be solved analytically, we will introduce the concept of minimizing a cost function to determine the optimal model parameters in a straight-forward manner.\r\nObjectives\r\nBy the end of this lesson, you will be able to\r\nexplain the role of the loss function in machine learning,\r\narticulate how to compute a regression on multiple variables,\r\nexplain the different statistical measures that quantify the quality of a regression,\r\nutilize categorical variables in a machine learning model, and\r\ncompute a linear regression model\r\nYou were introduced to the concept of linear regression by learning about simple linear regression. This initial approach treated linear regression as a statistical technique where the relation between independent variables (or features) and a dependent variable (or target) was determined by a mathematical relation. While powerful, the previous approach treated linear regression as a distinct, statistical approach to relating independent variables with a dependent variable. In this lesson, we instead treat linear regression as a machine learning task. As a result, we will use regression to fit a model to data. The model generated in this fashion can be explored in greater detail to either understand why the provided data follow the generated model (i.e., gain insight into the data), or the model can be used to generate new dependent values from future or unseen data (i.e., make predictions from the model).\r\nWe will use the tips data set. After loading this data, we display several rows, and next compute a simple linear regression to predict the tip feature from the total_bill feature.\r\nFor this section we need to load libraries:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(caret)\r\n\r\n\r\n\r\n\r\n\r\n#load data. \r\n#curl package lets us download data from a website with the proper location\r\n#check the packages tab and see if you have curl\r\n#try following\r\n  #?curl\r\n\r\nlibrary(curl)\r\n\r\nload(curl(\"https://raw.githubusercontent.com/Professor-Hunt/ACC8143/main/data/tips.rda\"))\r\n\r\n\r\n\r\n\r\n\r\nhead(tips,5)\r\n\r\n\r\n# A tibble: 5 x 7\r\n  total_bill   tip sex    smoker day   time    size\r\n       <dbl> <dbl> <chr>  <chr>  <chr> <chr>  <dbl>\r\n1       17.0  1.01 Female No     Sun   Dinner     2\r\n2       10.3  1.66 Male   No     Sun   Dinner     3\r\n3       21.0  3.5  Male   No     Sun   Dinner     3\r\n4       23.7  3.31 Male   No     Sun   Dinner     2\r\n5       24.6  3.61 Female No     Sun   Dinner     4\r\n\r\n#view the whole dataset\r\nknitr::kable(tips)%>%\r\n  kableExtra::kable_styling(\"striped\")%>%\r\n  kableExtra::scroll_box(width = \"100%\",height=\"300px\")\r\n\r\n\r\n\r\n\r\ntotal_bill\r\n\r\n\r\ntip\r\n\r\n\r\nsex\r\n\r\n\r\nsmoker\r\n\r\n\r\nday\r\n\r\n\r\ntime\r\n\r\n\r\nsize\r\n\r\n\r\n16.99\r\n\r\n\r\n1.01\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n10.34\r\n\r\n\r\n1.66\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n21.01\r\n\r\n\r\n3.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n23.68\r\n\r\n\r\n3.31\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n24.59\r\n\r\n\r\n3.61\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n25.29\r\n\r\n\r\n4.71\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n8.77\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n26.88\r\n\r\n\r\n3.12\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n15.04\r\n\r\n\r\n1.96\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n14.78\r\n\r\n\r\n3.23\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n10.27\r\n\r\n\r\n1.71\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n35.26\r\n\r\n\r\n5.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n15.42\r\n\r\n\r\n1.57\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n18.43\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n14.83\r\n\r\n\r\n3.02\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n21.58\r\n\r\n\r\n3.92\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n10.33\r\n\r\n\r\n1.67\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n16.29\r\n\r\n\r\n3.71\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n16.97\r\n\r\n\r\n3.50\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n20.65\r\n\r\n\r\n3.35\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n17.92\r\n\r\n\r\n4.08\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n20.29\r\n\r\n\r\n2.75\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n15.77\r\n\r\n\r\n2.23\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n39.42\r\n\r\n\r\n7.58\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n19.82\r\n\r\n\r\n3.18\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n17.81\r\n\r\n\r\n2.34\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n13.37\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n12.69\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n21.70\r\n\r\n\r\n4.30\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n19.65\r\n\r\n\r\n3.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n9.55\r\n\r\n\r\n1.45\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n18.35\r\n\r\n\r\n2.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n15.06\r\n\r\n\r\n3.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n20.69\r\n\r\n\r\n2.45\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n17.78\r\n\r\n\r\n3.27\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n24.06\r\n\r\n\r\n3.60\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n16.31\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n16.93\r\n\r\n\r\n3.07\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n18.69\r\n\r\n\r\n2.31\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n31.27\r\n\r\n\r\n5.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n16.04\r\n\r\n\r\n2.24\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n17.46\r\n\r\n\r\n2.54\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n13.94\r\n\r\n\r\n3.06\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n9.68\r\n\r\n\r\n1.32\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n30.40\r\n\r\n\r\n5.60\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n18.29\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n22.23\r\n\r\n\r\n5.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n32.40\r\n\r\n\r\n6.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n28.55\r\n\r\n\r\n2.05\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n18.04\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n12.54\r\n\r\n\r\n2.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n10.29\r\n\r\n\r\n2.60\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n34.81\r\n\r\n\r\n5.20\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n9.94\r\n\r\n\r\n1.56\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n25.56\r\n\r\n\r\n4.34\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n19.49\r\n\r\n\r\n3.51\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n38.01\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n26.41\r\n\r\n\r\n1.50\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n11.24\r\n\r\n\r\n1.76\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n48.27\r\n\r\n\r\n6.73\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n20.29\r\n\r\n\r\n3.21\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n13.81\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n11.02\r\n\r\n\r\n1.98\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n18.29\r\n\r\n\r\n3.76\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n17.59\r\n\r\n\r\n2.64\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n20.08\r\n\r\n\r\n3.15\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n16.45\r\n\r\n\r\n2.47\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n3.07\r\n\r\n\r\n1.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n1\r\n\r\n\r\n20.23\r\n\r\n\r\n2.01\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n15.01\r\n\r\n\r\n2.09\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n12.02\r\n\r\n\r\n1.97\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n17.07\r\n\r\n\r\n3.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n26.86\r\n\r\n\r\n3.14\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n25.28\r\n\r\n\r\n5.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n14.73\r\n\r\n\r\n2.20\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n10.51\r\n\r\n\r\n1.25\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n17.92\r\n\r\n\r\n3.08\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n27.20\r\n\r\n\r\n4.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n4\r\n\r\n\r\n22.76\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n17.29\r\n\r\n\r\n2.71\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n19.44\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n16.66\r\n\r\n\r\n3.40\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n10.07\r\n\r\n\r\n1.83\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n1\r\n\r\n\r\n32.68\r\n\r\n\r\n5.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n15.98\r\n\r\n\r\n2.03\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n34.83\r\n\r\n\r\n5.17\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n4\r\n\r\n\r\n13.03\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n18.28\r\n\r\n\r\n4.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n24.71\r\n\r\n\r\n5.85\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n21.16\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n28.97\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n22.49\r\n\r\n\r\n3.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n5.75\r\n\r\n\r\n1.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n16.32\r\n\r\n\r\n4.30\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n22.75\r\n\r\n\r\n3.25\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n40.17\r\n\r\n\r\n4.73\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n27.28\r\n\r\n\r\n4.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n12.03\r\n\r\n\r\n1.50\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n21.01\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n12.46\r\n\r\n\r\n1.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n11.35\r\n\r\n\r\n2.50\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n15.38\r\n\r\n\r\n3.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n44.30\r\n\r\n\r\n2.50\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n22.42\r\n\r\n\r\n3.48\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n20.92\r\n\r\n\r\n4.08\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n15.36\r\n\r\n\r\n1.64\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n20.49\r\n\r\n\r\n4.06\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n25.21\r\n\r\n\r\n4.29\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n18.24\r\n\r\n\r\n3.76\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n14.31\r\n\r\n\r\n4.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n14.00\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n7.25\r\n\r\n\r\n1.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n1\r\n\r\n\r\n38.07\r\n\r\n\r\n4.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n23.95\r\n\r\n\r\n2.55\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n25.71\r\n\r\n\r\n4.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n17.31\r\n\r\n\r\n3.50\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n29.93\r\n\r\n\r\n5.07\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n10.65\r\n\r\n\r\n1.50\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n12.43\r\n\r\n\r\n1.80\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n24.08\r\n\r\n\r\n2.92\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n4\r\n\r\n\r\n11.69\r\n\r\n\r\n2.31\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n13.42\r\n\r\n\r\n1.68\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n14.26\r\n\r\n\r\n2.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n15.95\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n12.48\r\n\r\n\r\n2.52\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n29.80\r\n\r\n\r\n4.20\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n6\r\n\r\n\r\n8.52\r\n\r\n\r\n1.48\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n14.52\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n11.38\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n22.82\r\n\r\n\r\n2.18\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n3\r\n\r\n\r\n19.08\r\n\r\n\r\n1.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n20.27\r\n\r\n\r\n2.83\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n11.17\r\n\r\n\r\n1.50\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n12.26\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n18.26\r\n\r\n\r\n3.25\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n8.51\r\n\r\n\r\n1.25\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n10.33\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n14.15\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n16.00\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n13.16\r\n\r\n\r\n2.75\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n17.47\r\n\r\n\r\n3.50\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n34.30\r\n\r\n\r\n6.70\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n6\r\n\r\n\r\n41.19\r\n\r\n\r\n5.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n5\r\n\r\n\r\n27.05\r\n\r\n\r\n5.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n6\r\n\r\n\r\n16.43\r\n\r\n\r\n2.30\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n8.35\r\n\r\n\r\n1.50\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n18.64\r\n\r\n\r\n1.36\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n3\r\n\r\n\r\n11.87\r\n\r\n\r\n1.63\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n9.78\r\n\r\n\r\n1.73\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n7.51\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n14.07\r\n\r\n\r\n2.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n13.13\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n17.26\r\n\r\n\r\n2.74\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n24.55\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n19.77\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n29.85\r\n\r\n\r\n5.14\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n5\r\n\r\n\r\n48.17\r\n\r\n\r\n5.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n6\r\n\r\n\r\n25.00\r\n\r\n\r\n3.75\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n13.39\r\n\r\n\r\n2.61\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n16.49\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n21.50\r\n\r\n\r\n3.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n12.66\r\n\r\n\r\n2.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n16.21\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n13.81\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n17.51\r\n\r\n\r\n3.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n24.52\r\n\r\n\r\n3.48\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n20.76\r\n\r\n\r\n2.24\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n31.71\r\n\r\n\r\n4.50\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n10.59\r\n\r\n\r\n1.61\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n10.63\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n50.81\r\n\r\n\r\n10.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n15.81\r\n\r\n\r\n3.16\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n7.25\r\n\r\n\r\n5.15\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n31.85\r\n\r\n\r\n3.18\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n16.82\r\n\r\n\r\n4.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n32.90\r\n\r\n\r\n3.11\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n17.89\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n14.48\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n9.60\r\n\r\n\r\n4.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n34.63\r\n\r\n\r\n3.55\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n34.65\r\n\r\n\r\n3.68\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n23.33\r\n\r\n\r\n5.65\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n45.35\r\n\r\n\r\n3.50\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n23.17\r\n\r\n\r\n6.50\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n40.55\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n20.69\r\n\r\n\r\n5.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n5\r\n\r\n\r\n20.90\r\n\r\n\r\n3.50\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n30.46\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n5\r\n\r\n\r\n18.15\r\n\r\n\r\n3.50\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n23.10\r\n\r\n\r\n4.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n15.69\r\n\r\n\r\n1.50\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSun\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n19.81\r\n\r\n\r\n4.19\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n28.44\r\n\r\n\r\n2.56\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n15.48\r\n\r\n\r\n2.02\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n16.58\r\n\r\n\r\n4.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n7.56\r\n\r\n\r\n1.44\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n10.34\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n43.11\r\n\r\n\r\n5.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n4\r\n\r\n\r\n13.00\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n13.51\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n18.71\r\n\r\n\r\n4.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n3\r\n\r\n\r\n12.74\r\n\r\n\r\n2.01\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n13.00\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n16.40\r\n\r\n\r\n2.50\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n20.53\r\n\r\n\r\n4.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n4\r\n\r\n\r\n16.47\r\n\r\n\r\n3.23\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nThur\r\n\r\n\r\nLunch\r\n\r\n\r\n3\r\n\r\n\r\n26.59\r\n\r\n\r\n3.41\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n38.73\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n24.27\r\n\r\n\r\n2.03\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n12.76\r\n\r\n\r\n2.23\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n30.06\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n25.89\r\n\r\n\r\n5.16\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n48.33\r\n\r\n\r\n9.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n13.27\r\n\r\n\r\n2.50\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n28.17\r\n\r\n\r\n6.50\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n12.90\r\n\r\n\r\n1.10\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n28.15\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n5\r\n\r\n\r\n11.59\r\n\r\n\r\n1.50\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n7.74\r\n\r\n\r\n1.44\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n30.14\r\n\r\n\r\n3.09\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n12.16\r\n\r\n\r\n2.20\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n13.42\r\n\r\n\r\n3.48\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n8.58\r\n\r\n\r\n1.92\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nLunch\r\n\r\n\r\n1\r\n\r\n\r\n15.98\r\n\r\n\r\n3.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nFri\r\n\r\n\r\nLunch\r\n\r\n\r\n3\r\n\r\n\r\n13.42\r\n\r\n\r\n1.58\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n16.27\r\n\r\n\r\n2.50\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n10.09\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nFri\r\n\r\n\r\nLunch\r\n\r\n\r\n2\r\n\r\n\r\n20.45\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n13.28\r\n\r\n\r\n2.72\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n22.12\r\n\r\n\r\n2.88\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n24.01\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n4\r\n\r\n\r\n15.69\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n11.61\r\n\r\n\r\n3.39\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n10.77\r\n\r\n\r\n1.47\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n15.53\r\n\r\n\r\n3.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n10.07\r\n\r\n\r\n1.25\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n12.60\r\n\r\n\r\n1.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n32.83\r\n\r\n\r\n1.17\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n35.83\r\n\r\n\r\n4.67\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n29.03\r\n\r\n\r\n5.92\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n3\r\n\r\n\r\n27.18\r\n\r\n\r\n2.00\r\n\r\n\r\nFemale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n22.67\r\n\r\n\r\n2.00\r\n\r\n\r\nMale\r\n\r\n\r\nYes\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n17.82\r\n\r\n\r\n1.75\r\n\r\n\r\nMale\r\n\r\n\r\nNo\r\n\r\n\r\nSat\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n18.78\r\n\r\n\r\n3.00\r\n\r\n\r\nFemale\r\n\r\n\r\nNo\r\n\r\n\r\nThur\r\n\r\n\r\nDinner\r\n\r\n\r\n2\r\n\r\n\r\n\r\nPerform simple linear regression\r\n\r\n\r\nOLS1<-lm(formula=tip~total_bill,data=tips)\r\n#general output\r\nOLS1\r\n\r\n\r\n\r\nCall:\r\nlm(formula = tip ~ total_bill, data = tips)\r\n\r\nCoefficients:\r\n(Intercept)   total_bill  \r\n     0.9203       0.1050  \r\n\r\n#more common output\r\nsummary(OLS1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = tip ~ total_bill, data = tips)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-3.1982 -0.5652 -0.0974  0.4863  3.7434 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 0.920270   0.159735   5.761 2.53e-08 ***\r\ntotal_bill  0.105025   0.007365  14.260  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.022 on 242 degrees of freedom\r\nMultiple R-squared:  0.4566,    Adjusted R-squared:  0.4544 \r\nF-statistic: 203.4 on 1 and 242 DF,  p-value: < 2.2e-16\r\n\r\n#correlation or R-squared...\r\ncor(tips$total_bill,tips$tip)^2\r\n\r\n\r\n[1] 0.4566166\r\n\r\nFormalism\r\nFormally, this simple linear model related the independent variables \\(x_i\\) to the dependent variables \\(y_i\\) in our data set via two parameters: an intercept, and a slope. Mathematically, we express this relation in the following form:\r\n\\[\r\nf(x_i) = \\beta * x_i + \\alpha + \\epsilon_i\r\n\\]\r\nwhere \\(\\epsilon_i\\) accounts for the difference between the model and the data for each data point \\((x_i,y_i)\\). If we have a perfect model, these errors, \\(\\epsilon_i\\), are all zero, and \\(y_i = f(x_i)\\). In real life, however, the error terms rarely vanish because even if the original relationship is perfect noise creeps into the measurement process.\r\nAs a result, in this simple example we wish to determine the model parameters: \\(\\beta_i\\), and \\(\\alpha_i\\) that minimize the values of \\(\\epsilon_i\\). We could perform this process in an iterative manner, trying different values for the model parameters and measuring the error function. This approach is often used in machine learning, where we define a cost function that we seek to minimize by selecting the best model parameters.\r\nIn the case of a simple linear model, we have several potential cost (or loss) functions that we could seek to minimize, but we will use the common l2-norm: \\(\\epsilon_i^2 = \\left( \\ y_i - f(x_i) \\ \\right)^2\\), where \\(f(x_i)\\) is defined by our model parameters. We demonstrate this approach visually in the following code block, where we minimize the sum of the l2-norm model residuals, which is done by finding the best model parameters: \\(\\hat{\\beta}\\), and \\(\\hat{\\alpha}\\).\r\n\r\n\r\n#Get some data\r\nAnsDat<-anscombe%>%\r\n  select(y1,x1)\r\n\r\n#extract x and y columns\r\nY<-AnsDat$y1\r\nX<-AnsDat$x1\r\n\r\n#find the number of data points\r\nn<-nrow(AnsDat)\r\n\r\n#determine mean values\r\nmean_x<-mean(X,na.rm = TRUE)\r\nmean_y<-mean(Y,na.rm = TRUE)\r\n\r\n#determine best fit model parameters (from simple linear regression)\r\nbeta = sum((X - mean_x) * (Y - mean_y)) / sum((X - mean_x)**2)\r\nbeta\r\n\r\n\r\n[1] 0.5000909\r\n\r\nalpha = mean_y - beta * mean_x\r\nalpha\r\n\r\n\r\n[1] 3.000091\r\n\r\n#lets double check\r\nsummary(lm(formula=Y~X,data=AnsDat))\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Y ~ X, data = AnsDat)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)   \r\n(Intercept)   3.0001     1.1247   2.667  0.02573 * \r\nX             0.5001     0.1179   4.241  0.00217 **\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.237 on 9 degrees of freedom\r\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \r\nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\r\n\r\nPlots\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\n#create regression plot\r\nggplot(AnsDat,aes(x1, y1)) +\r\n  geom_point() +\r\n  geom_smooth(method='lm', se=FALSE) +\r\n  geom_segment(aes(x=X, xend=X, y=Y, yend=lm(Y~X)$fitted.values, color=\"error\"))+\r\n  theme_minimal() +\r\n  labs(x='X Values', y='Y Values', title='Linear Regression Plot') +\r\n  theme(plot.title = element_text(hjust=0.5, size=20, face='bold')) + \r\n  theme(legend.title = element_blank())\r\n\r\n\r\n\r\n\r\nCost Function\r\nThis simple example demonstrates a fundamental concept in machine learning, namely the minimization of a cost (or loss) function, which quantifies how well a model represents a data set. For a given data set, the cost function is completely specified by the model parameters, thus a more complex model has a more complex cost function, which can become difficult to minimize. To clarify this point, we now turn to the exploration of the shape of cost functions.\r\nFor simplicity, we start with a one-dimensional cost function, a linear model with no intercept: \\(f(x_i) = \\beta x_i\\). In the following code cell, we compute the cost function for a given data set as a function of the unknown parameter \\(\\beta\\). In this case, the minimum is easy to visualize, given the steepness of the cost function around the minimum.\r\n\r\n\r\n#define our betas\r\nbetas<-seq(-4,4,length.out=100)\r\n\r\n#define our cost function\r\nl2n = sapply(as.matrix(betas), function(m) log(sqrt(sum((as.matrix(tips$tip) - m*as.matrix(tips$total_bill))^2))))  # The L2-norm\r\n\r\n\r\n\r\n\r\n\r\nlibrary(ggplot2)\r\ncostplot<-as.data.frame(cbind(betas,l2n))\r\n#create regression plot\r\nggplot(costplot,aes(betas, l2n)) +\r\n  geom_point(color=\"blue\") + geom_line()+\r\n  geom_vline(xintercept=0, color=\"red\")\r\n\r\n\r\n\r\n\r\nIn general, however, we face two challenges:\r\nthe cost function will likely be more complex, and\r\nour data will be higher dimensional.\r\nIn general, we must employ a (potentially) complex mathematical technique to find the (hopefully) global minimum of the cost function. We can increase the complexity of our cost function analysis by extending the original model to include both a slope and an intercept. We now must find the minimum of this two dimensional model, given our observed data. We do this in the following code cell where we generate a grid of values in our two parameters, and compute the cost function for these different parameter combinations.\r\nTo display the data which generates a sampling grid across potential values for the slope \\(\\beta\\) and intercept \\(\\alpha\\) in our model. We once again vectorize our cost function and broadcast it across the sampling grid. We accumulate the cost at each grid point and generate a two-dimensional image of the values of the cost function across our sampling grid. To make the image appear cleaner, we perform Gaussian interpolation between sample points.\r\nAs the following two-dimensional image displays, our cost function is not aligned with either parameter, but is steeper in the slope parameter and less steep in the intercept parameter. Thus, we would expect that small changes in the slope will quickly increase our cost (which we saw in the previous one-dimensional example), while small changes in the intercept will produce smaller changes in our cost function (note that the range for intercepts is much larger than the range for the slope parameters).\r\n\r\n\r\n#define our betas\r\nbetas<-seq(-4,4,length.out=100)\r\nalphas<-seq(-40,40,length.out=100)\r\n\r\n#define our cost function\r\nl2n2 = mapply( function(m,b) log(sqrt(sum((as.matrix(tips$tip) - m*as.matrix(tips$total_bill) - b)^2))),as.matrix(betas),as.matrix(alphas))  # The L2-norm\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nAs we move to higher dimensional data sets or more complex cost functions, the challenge of finding the global minimum becomes increasingly difficult. As a result, many mathematical techniques have been developed to find the global minimum of a (potentially) complex function. The standard approach is gradient descent, where we use the fact that the first derivative (or gradient) measures the slope of a function at a given point. We can use the slope to infer which direction is downhill and thus travel (hopefully) towards the minimum.\r\nA major challenge with this approach is the potential to become stuck in a local and not global minima. Thus, modifications are often added to reduce the likelihood of becoming stuck in a local minimum. One popular example of this approach is known as stochastic gradient descent. This algorithm employs standard gradient descent, but adds an occasional random jump in the parameter space to reduce the chances of being stuck in a local valley. Another, very different, approach to this problem is the use of genetic algorithms, which employ techniques from evolutionary biology to minimize the cost function.\r\nFor a mental picture of this process, imagine hiking in the mountains and flip the challenge to finding the highest peak, so we will use gradient ascent. Gradient ascent is similar to finding the local mountain peak and climbing it. This local peak might look like it is the largest, but a random jump away from the local peak might enable one to view much larger peaks beyond, which can subsequently be climbed with a new gradient ascent.\r\nWhenever you perform machine learning in the future, you should keep in mind that the model that you generate for a given data set has generally resulted from the minimization of a cost function. Thus, there remains the possibility that with more effort, more data, or a better cost minimization strategy, a new, and better model may potentially exist.\r\n\r\n\r\n\r\n#what is a function?\r\nrmse = function(actual, predicted) {\r\n  sqrt(mean((actual - predicted) ^ 2))\r\n}\r\n\r\n\r\n\r\n\r\n\r\n#get our samples\r\n\r\n#lets split the data 60/40\r\nlibrary(caret)\r\ntrainIndex <- createDataPartition(tips$tip, p = .6, list = FALSE, times = 1)\r\n\r\n#look at the first few\r\n#head(trainIndex)\r\n\r\n#grab the data\r\ntipsTrain <- tips[ trainIndex,]\r\ntipsTest  <- tips[-trainIndex,]\r\n\r\n\r\n\r\nLinear Regression\r\nIn the following code cells, we use the lm estimator to fit our sample data, plot the results, and finally display the fit coefficients.\r\nThe first code cell defines a function that will make two plots. The top plot is a comparison between a single independent variable (Total Bill) and the dependent variable (Tip). This plot differentiates the training data, the testing data, and the linear model. The bottom plot displays the model residuals (dependent variable - model result) as a function of the independent variable. The primary benefit of this plot is the ability to identify any structure in the residuals, which can indicate a bad model. For example, if the residual plot shows a linear relationship, that indicates the original model incorrectly related the independent and dependent variables.\r\nIn the following code cells, we first compute a linear fit with no intercept, after which we compute a linear fit with both a slope and an intercept. The fit results are displayed as well as the regression and residual plots.\r\n\r\n\r\n#fit simple linear regression model\r\nmodel_noint <- lm(tip ~ 0+total_bill , data = tips)\r\n\r\n#create scatterplot of data\r\nplot(tips$total_bill, tips$tip)\r\n\r\n#add fitted regression line\r\nabline(model_noint)\r\n\r\n\r\n\r\n\r\nRecap\r\n\r\nThe is borrowed content from https://insights.principa.co.za/4-types-of-data-analytics-descriptive-diagnostic-predictive-prescriptive‚Ü©Ô∏é\r\nThis is a narrow definition because you can also impute with summary statistics such as mean or median.‚Ü©Ô∏é\r\nContent for this caret portion is borrowed from https://www.rebeccabarter.com/blog/2017-11-17-caret_tutorial/‚Ü©Ô∏é\r\nhttps://data.library.virginia.edu/is-r-squared-useless/‚Ü©Ô∏é\r\nhttps://www.r-bloggers.com/2013/11/computing-and-visualizing-pca-in-r/‚Ü©Ô∏é\r\n",
      "last_modified": "2021-10-18T11:15:06-05:00"
    }
  ],
  "collections": []
}
